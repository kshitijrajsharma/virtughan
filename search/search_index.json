{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"virtughan","text":"<p>Name is combination of two words <code>virtual</code> &amp; <code>cube</code> , where <code>cube</code> translated to Nepali word <code>\u0918\u0928</code>,  also known as virtual computation cube.</p> <p>Demo : https://virtughan.live/ </p>"},{"location":"#install","title":"Install","text":"<p>As a python package : </p> <p>https://pypi.org/project/virtughan/ </p> <pre><code>pip install virtughan\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Follow Notebook Here</p>"},{"location":"#background","title":"Background","text":"<p>We started initially by looking at how Google Earth Engine (GEE) computes results on-the-fly at different zoom levels on large-scale Earth observation datasets. We were fascinated by the approach and felt an urge to replicate something similar on our own in an open-source manner. We knew Google uses their own kind of tiling, so we started from there.</p> <p>Initially, we faced a challenge \u2013 how could we generate tiles and compute at the same time without pre-computing the whole dataset? Pre-computation would lead to larger processed data sizes, which we didn\u2019t want. And so, the exploration began and the concept of on the fly tiling computation introduced </p> <p>At university, we were introduced to the concept of data cubes and the advantages of having a time dimension and semantic layers in the data. It seemed fascinating, despite the challenge of maintaining terabytes of satellite imagery. We thought \u2013 maybe we could achieve something similar by developing an approach where one doesn\u2019t need to replicate data but can still build a data cube with semantic layers and computation. This raised another challenge \u2013 how to make it work? And hence come the virtual data cube</p> <p>We started converting Sentinel-2 images to Cloud Optimized GeoTIFFs (COGs) and experimented with the time dimension using Python\u2019s xarray to compute the data. We found that earth-search\u2019s effort to store Sentinel images as COGs made it easier for us to build virtual data cubes across the world without storing any data. This felt like an achievement and proof that modern data cubes should focus on improving computation rather than worrying about how to manage terabytes of data.</p> <p>We wanted to build something to show that this approach actually works and is scalable. We deliberately chose to use only our laptops to run the prototype and process a year\u2019s worth of data without expensive servers.</p> <p>Learn about COG and how to generate one for this project Here</p>"},{"location":"#purpose","title":"Purpose","text":""},{"location":"#1-efficient-on-the-fly-tile-computation","title":"1. Efficient On-the-Fly Tile Computation","text":"<p>This research explores how to perform real-time calculations on satellite images at different zoom levels, similar to Google Earth Engine, but using open-source tools. By using Cloud Optimized GeoTIFFs (COGs) with Sentinel-2 imagery, large images can be analyzed without needing to pre-process or store them. The study highlights how this method can scale well and work efficiently, even with limited hardware. Our main focus is on how to scale the computation on different zoom-levels without introducing server overhead </p> <p>Watch</p>"},{"location":"#example-python-usage","title":"Example python usage","text":"<pre><code>import mercantile\nfrom PIL import Image\nfrom io import BytesIO\nfrom virtughan.tile import TileProcessor\n\nlat, lon = 28.28139, 83.91866\nzoom_level = 12\nx, y, z = mercantile.tile(lon, lat, zoom_level)\n\ntile_processor = TileProcessor()\n\nimage_bytes, feature = await tile_processor.cached_generate_tile(\n    x=x,\n    y=y,\n    z=z,\n    start_date=\"2020-01-01\",\n    end_date=\"2025-01-01\",\n    cloud_cover=30,\n    band1=\"red\",\n    band2=\"nir\",\n    formula=\"(band2-band1)/(band2+band1)\",\n    colormap_str=\"RdYlGn\",\n)\n\nimage = Image.open(BytesIO(image_bytes))\n\nprint(f\"Tile: {x}_{y}_{z}\")\nprint(f\"Date: {feature['properties']['datetime']}\")\nprint(f\"Cloud Cover: {feature['properties']['eo:cloud_cover']}%\")\n\nimage.save(f'tile_{x}_{y}_{z}.png')\n</code></pre>"},{"location":"#2-virtual-computation-cubes-focusing-on-computation","title":"2. Virtual Computation Cubes: Focusing on Computation","text":"<p>While storing large images can offer some benefits, we believe that placing emphasis on efficient computation yields far greater advantages and effectively removes the need to worry about large-scale image storage. COGs make it possible to analyze images directly without storing the entire dataset. This introduces the idea of virtual computation cubes, where images are stacked and processed over time, allowing for analysis across different layers ( including semantic layers ) without needing to download or save everything. So original data is never replicated. In this setup, a data provider can store and convert images to COGs, while users or service providers focus on calculations. This approach reduces the need for terra-bytes of storage and makes it easier to process large datasets quickly.</p>"},{"location":"#example-python-usage_1","title":"Example python usage","text":"<p>Example NDVI calculation </p> <pre><code>from virtughan.engine import VirtughanProcessor\n\nprocessor = VirtughanProcessor(\n    bbox=[83.84765625, 28.22697003891833, 83.935546875, 28.304380682962773],\n    start_date=\"2023-01-01\",\n    end_date=\"2025-01-01\",\n    cloud_cover=30,\n    formula=\"(band2-band1)/(band2+band1)\",\n    band1=\"red\",\n    band2=\"nir\",\n    operation=\"median\",\n    timeseries=True,\n    output_dir=\"virtughan_output\",\n    workers=16\n)\n\nprocessor.compute()\n</code></pre>"},{"location":"#summary","title":"Summary","text":"<p>This research introduces methods on how to use COGs, the SpatioTemporal Asset Catalog (STAC) API, and NumPy arrays to improve the way large Earth observation datasets are accessed and processed. The method allows users to focus on specific areas of interest, process data across different bands and layers over time, and maintain optimal resolution while ensuring fast performance. By using the STAC API, it becomes easier to search for and only process the necessary data without needing to download entire images ( not even the single scene , only accessing the parts ) The study shows how COGs can improve the handling of large datasets, not only making  the access faster but also making computation efficient, and scalable across different zoom levels . </p> <p></p>"},{"location":"#sample-case-study","title":"Sample case study :","text":"<p>Watch Video</p>"},{"location":"#local-setup","title":"Local Setup","text":"<p>This project has FASTAPI and Plain JS Frontend.</p> <p>Inorder to setup project , follow here</p>"},{"location":"#tech-stack","title":"Tech Stack","text":""},{"location":"#resources-and-credits","title":"Resources and Credits","text":"<ul> <li>https://registry.opendata.aws/sentinel-2-l2a-cogs/ COGS Stac API for sentinel-2</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>Liked the concept? Want to be part of it ?  </p> <p>If you have experience with JavaScript, FastAPI, building geospatial Python libraries , we\u2019d love your contributions! But you don\u2019t have to be a coder to help\u2014spreading the word is just as valuable.  </p>"},{"location":"#how-you-can-contribute","title":"How You Can Contribute ?","text":"<p>Code Contributions - Fork the repository and submit a PR with improvements, bug fixes, or features. Use commitizen for your commits - Help us refine our development guidelines !  </p> <p>Documentation &amp; Testing - Improve our docs to make it easier for others to get started. - Test features and report issues to help us build a robust system.  </p> <p>Spread the Word - Share the project on social media or among developer communities. - Bring in more contributors who might be interested!  </p> <p>Support Us If you love what we\u2019re building, consider buying us a coffee \u2615 to keep the project going!  </p> <p> </p>"},{"location":"#acknowledgment","title":"Acknowledgment","text":"<p>This project was initiated during the project work of our master's program , Coopernicus Masters in Digital Earth.  We are thankful to all those involved and supported us from the program. </p> <p> </p>"},{"location":"#copyright","title":"Copyright","text":"<p>\u00a9 2024 \u2013 Concept by Kshitij and Upen , Distributed under GNU General Public License v3.0 </p>"},{"location":"cog/","title":"Learn about COG","text":""},{"location":"cog/#understand-cog","title":"Understand COG","text":""},{"location":"cog/#cloud-optimized-geotiff-cog","title":"Cloud Optimized GeoTIFF (COG)","text":"<p>A Cloud Optimized GeoTIFF (COG) is a GeoTIFF file that has been optimized for efficient access and processing in cloud environments. It allows for efficient reading of small portions of the file without requiring the entire file to be downloaded, making it ideal for use in web applications and cloud-based geographic information systems (GIS).</p>"},{"location":"cog/#key-features-of-cog","title":"Key Features of COG","text":"<ol> <li> <p>Internal Tiling: The image data is divided into regular tiles, typically 256x256 pixels. This tiling allows for efficient access to small parts of the image, which is particularly useful for operations like map tiling and zooming.</p> </li> <li> <p>Overviews (Pyramids): These are reduced resolution versions of the image that allow for faster access at different zoom levels. Overviews are stored within the same file and can be accessed quickly to display lower resolution images when high resolution is not necessary.</p> </li> <li> <p>Efficient Data Access: COGs are structured to allow HTTP range requests, enabling clients to request just the portions of the file they need. This makes it possible to read a small part of the image without downloading the entire file.</p> </li> <li> <p>Metadata: COGs include metadata that describes the internal tiling and overviews, enabling clients to efficiently locate and access the required data.</p> </li> </ol>"},{"location":"cog/#how-cog-works-internally","title":"How COG Works Internally","text":""},{"location":"cog/#1-internal-tiling","title":"1. Internal Tiling","text":"<ul> <li> <p>Tiles: The image is divided into regular, non-overlapping tiles (e.g., 256x256 pixels). Each tile can be accessed independently, allowing for efficient read operations on small portions of the image.</p> </li> <li> <p>Tile Indexing: The tiles are indexed within the file, allowing for quick location and retrieval of specific tiles.</p> </li> </ul>"},{"location":"cog/#2-overviews-pyramids","title":"2. Overviews (Pyramids)","text":"<ul> <li> <p>Purpose: Overviews provide lower resolution versions of the image. They are used to quickly access and display the image at different zoom levels without processing the full-resolution data.</p> </li> <li> <p>Levels: Overviews are created at multiple levels of reduced resolution. For example, a 1:4 overview would represent the image at one-fourth the resolution of the original.</p> </li> <li> <p>Storage: Overviews are stored within the same GeoTIFF file and can be accessed through the same indexing mechanism as the full-resolution tiles.</p> </li> </ul>"},{"location":"cog/#3-efficient-data-access","title":"3. Efficient Data Access","text":"<ul> <li> <p>HTTP Range Requests: COGs support HTTP range requests, which allow clients to request specific byte ranges from the file. This is particularly useful for accessing individual tiles or overviews without downloading the entire file.</p> </li> <li> <p>Indexing Metadata: Metadata within the COG provides information about the structure of the file, including the location of tiles and overviews. This metadata is used by clients to efficiently locate and read the required data.</p> </li> </ul>"},{"location":"cog/#tiling-and-overview-example","title":"Tiling and Overview Example","text":"<p>Consider an example where we have a high-resolution satellite image stored as a COG:</p> <ul> <li>High-Resolution Image: The original image is 10,000 x 10,000 pixels.</li> <li>Tiles: The image is divided into 256x256 pixel tiles.</li> <li>Overviews: Overviews are created at multiple levels (e.g., 1:2, 1:4, 1:8).</li> </ul> <p>When a client requests a view of the image at a low zoom level, the client can: 1. Access Overviews: Fetch the appropriate overview level (e.g., 1:8) to quickly display a low-resolution version of the image. 2. Access Tiles: Request specific tiles from the full-resolution image as needed for detailed views.</p>"},{"location":"cog/#benefits-of-cog","title":"Benefits of COG","text":"<ul> <li>Performance: Faster data access and reduced bandwidth usage due to efficient tile and overview retrieval.</li> <li>Scalability: Ideal for cloud environments where data is accessed over the network.</li> <li>Flexibility: Supports a wide range of applications, from web mapping to scientific analysis.</li> </ul>"},{"location":"cog/#how-to-generate-cog-for-this-project","title":"How to Generate  COG for this project ?","text":"<p>Sentinel 2 Raw Image  </p> <p>Work on 10m resolution </p> <p>Get all the jp2 images </p> <pre><code>ls -1 *.jp2 &gt; jp2_list.txt\n</code></pre> <p>Merge</p> <pre><code>gdal_merge.py -separate --optfile jp2_list.txt -o sentinel_r10.tif \n</code></pre> <p>Or</p> <pre><code>gdal_merge.py -separate T45RTM_20241225T050129_B02_10m.jp2 T45RTM_20241225T050129_B03_10m.jp2 T45RTM_20241225T050129_B04_10m.jp2 T45RTM_20241225T050129_B08_10m.jp2 -o sentinel210m.tif -a_nodata 0\n</code></pre> <p>Check :  <pre><code>gdalinfo sentinel_r10.tif\n</code></pre></p> <p></p> <p>Size : </p> <p>2.1 GB for the merged file </p> <p>Conversion to COG : </p> <pre><code>gdal_translate -of COG sentinel_r10.tif sentinel_r10_cog.tif  -co NUM_THREADS=32 -co COMPRESS=DEFLATE -co BIGTIFF=YES -co TILING_SCHEME=GoogleMapsCompatible -co LEVEL=9\n</code></pre> <p>GDAL info</p> <p><pre><code> gdalinfo sentinel_r10_cog.tif\n</code></pre> </p>"},{"location":"docs/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"docs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"docs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>poetry </li> </ul>"},{"location":"install/#install-poetry","title":"Install Poetry","text":"<p>If you don't have poetry installed, you can install it using the following command:</p> <pre><code>pip install poetry\n</code></pre>"},{"location":"install/#install","title":"Install","text":"<pre><code>poetry install\n</code></pre>"},{"location":"install/#run","title":"Run","text":"<pre><code>poetry run uvicorn API:app --workers 2\n</code></pre>"},{"location":"examples/usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install virtughan==0.7.0\n</pre> !pip install virtughan==0.7.0 In\u00a0[9]: Copied! <pre># Define the bounding box\nbbox =  [-17.942562103271488,28.569682805210718,-17.839050292968754,28.64689695054486] # cumbre vieja volcano spain area\n\n# Define the parameters\nstart_date = \"2021-10-01\"\nend_date = \"2021-12-30\"\ncloud_cover = 10\nformula = \"(band1-band2)/(band1+band2)\"  # NBR\nband1 = \"nir08\"\nband2 = \"swir22\"\noperation = \"max\"\noutput_dir = \"virtughan_output\"\ntimeseries = True\nworkers = 1 # no of parallel workers\nsmart_filter=False\n</pre> # Define the bounding box bbox =  [-17.942562103271488,28.569682805210718,-17.839050292968754,28.64689695054486] # cumbre vieja volcano spain area  # Define the parameters start_date = \"2021-10-01\" end_date = \"2021-12-30\" cloud_cover = 10 formula = \"(band1-band2)/(band1+band2)\"  # NBR band1 = \"nir08\" band2 = \"swir22\" operation = \"max\" output_dir = \"virtughan_output\" timeseries = True workers = 1 # no of parallel workers smart_filter=False In\u00a0[10]: Copied! <pre>import shutil , os\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\n</pre> import shutil , os if os.path.exists(output_dir):     shutil.rmtree(output_dir) In\u00a0[11]: Copied! <pre>from virtughan.engine import VirtughanProcessor\n\nprocessor = VirtughanProcessor(\n    bbox,\n    start_date,\n    end_date,\n    cloud_cover,\n    formula,\n    band1,\n    band2,\n    operation,\n    timeseries,\n    output_dir,\n    workers=workers,\n    smart_filter=smart_filter,\n    cmap=\"RdYlGn\"\n)\nprocessor.compute()\n</pre> from virtughan.engine import VirtughanProcessor  processor = VirtughanProcessor(     bbox,     start_date,     end_date,     cloud_cover,     formula,     band1,     band2,     operation,     timeseries,     output_dir,     workers=workers,     smart_filter=smart_filter,     cmap=\"RdYlGn\" ) processor.compute() <pre>Engine starting...\nSearching STAC .....\nTotal scenes found: 11\nScenes covering input area: 11\nScenes after removing overlaps: 6\nComputing Band Calculation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:07&lt;00:00,  1.27s/it]\nAggregating results...\nSaving aggregated result with colormap...\nCreating GIF and zipping TIFF files...\nSaved timeseries GIF to virtughan_output/output.gif\nSaved intermediate images ZIP to virtughan_output/tiff_files.zip\n</pre> In\u00a0[12]: Copied! <pre>from IPython.display import Image\nImage(open(os.path.join(output_dir,'output.gif'),'rb').read())\n</pre> from IPython.display import Image Image(open(os.path.join(output_dir,'output.gif'),'rb').read()) Out[12]: In\u00a0[15]: Copied! <pre>import mercantile\nfrom PIL import Image\nfrom io import BytesIO\nfrom virtughan.tile import TileProcessor\n\n# Define the parameters\nlat, lon = 28.28139, 83.91866\nzoom_level = 12\nx, y, z = mercantile.tile(lon, lat, zoom_level)\n\ntile_processor = TileProcessor()\n\nimage_bytes, feature = await tile_processor.cached_generate_tile(\n    x=x,\n    y=y,\n    z=z,\n    start_date=\"2020-01-01\",\n    end_date=\"2025-01-01\",\n    cloud_cover=30,\n    band1=\"red\",\n    band2=\"nir\",\n    formula=\"(band2-band1)/(band2+band1)\",\n    colormap_str=\"RdYlGn\",\n)\n\nimage = Image.open(BytesIO(image_bytes))\n\nprint(f\"Tile: {x}_{y}_{z}\")\nprint(f\"Date: {feature['properties']['datetime']}\")\nprint(f\"Cloud Cover: {feature['properties']['eo:cloud_cover']}%\")\n\nimage.save(f'tile_{x}_{y}_{z}.png')\n</pre> import mercantile from PIL import Image from io import BytesIO from virtughan.tile import TileProcessor  # Define the parameters lat, lon = 28.28139, 83.91866 zoom_level = 12 x, y, z = mercantile.tile(lon, lat, zoom_level)  tile_processor = TileProcessor()  image_bytes, feature = await tile_processor.cached_generate_tile(     x=x,     y=y,     z=z,     start_date=\"2020-01-01\",     end_date=\"2025-01-01\",     cloud_cover=30,     band1=\"red\",     band2=\"nir\",     formula=\"(band2-band1)/(band2+band1)\",     colormap_str=\"RdYlGn\", )  image = Image.open(BytesIO(image_bytes))  print(f\"Tile: {x}_{y}_{z}\") print(f\"Date: {feature['properties']['datetime']}\") print(f\"Cloud Cover: {feature['properties']['eo:cloud_cover']}%\")  image.save(f'tile_{x}_{y}_{z}.png')  <pre>Tile: 3002_1712_12\nDate: 2024-12-30T05:10:56.883000Z\nCloud Cover: 1.140427%\n</pre> In\u00a0[16]: Copied! <pre>from IPython.display import Image as display_image\ndisplay_image(f'tile_{x}_{y}_{z}.png')\n</pre> from IPython.display import Image as display_image display_image(f'tile_{x}_{y}_{z}.png')  Out[16]: In\u00a0[17]: Copied! <pre>import os\n\nbbox = [83.84765625, 28.22697003891833, 83.935546875, 28.304380682962773]\nstart_date = \"2024-12-15\"\nend_date = \"2024-12-31\"\ncloud_cover = 30\nbands_list = [\"blue\",\"green\",\"red\", \"nir\",\"swir16\",\"swir22\"] # order of the bands will result order in the resulting output\noutput_dir = \"./sentinel_images\"\nworkers = 1\n\nos.makedirs(output_dir, exist_ok=True)\n</pre> import os  bbox = [83.84765625, 28.22697003891833, 83.935546875, 28.304380682962773] start_date = \"2024-12-15\" end_date = \"2024-12-31\" cloud_cover = 30 bands_list = [\"blue\",\"green\",\"red\", \"nir\",\"swir16\",\"swir22\"] # order of the bands will result order in the resulting output output_dir = \"./sentinel_images\" workers = 1  os.makedirs(output_dir, exist_ok=True) In\u00a0[18]: Copied! <pre>from virtughan.extract import ExtractProcessor, VALID_BANDS\n\nprint(\"Available bands: \", VALID_BANDS)\n\nextractor = ExtractProcessor(\n    bbox,\n    start_date,\n    end_date,\n    cloud_cover,\n    bands_list,\n    output_dir,\n    workers=workers,\n)\nextractor.extract()\n</pre> from virtughan.extract import ExtractProcessor, VALID_BANDS  print(\"Available bands: \", VALID_BANDS)  extractor = ExtractProcessor(     bbox,     start_date,     end_date,     cloud_cover,     bands_list,     output_dir,     workers=workers, ) extractor.extract() <pre>Available bands:  {'red': 'Red - 10m', 'green': 'Green - 10m', 'blue': 'Blue - 10m', 'nir': 'NIR 1 - 10m', 'swir22': 'SWIR 2.2\u03bcm - 20m', 'rededge2': 'Red Edge 2 - 20m', 'rededge3': 'Red Edge 3 - 20m', 'rededge1': 'Red Edge 1 - 20m', 'swir16': 'SWIR 1.6\u03bcm - 20m', 'wvp': 'Water Vapour (WVP)', 'nir08': 'NIR 2 - 20m', 'aot': 'Aerosol optical thickness (AOT)', 'coastal': 'Coastal - 60m', 'nir09': 'NIR 3 - 60m'}\nExtracting bands...\nTotal scenes found: 4\nScenes covering input area: 4\nScenes after removing overlaps: 4\nFilter from : 2024-12-15 to : 2024-12-30\nSelecting 1 image per 4 days\nScenes after applying smart filter: 4\nExtracting Bands:   0%|          | 0/4 [00:00&lt;?, ?it/s]Stacking Bands...\nExtracting Bands:  25%|\u2588\u2588\u258c       | 1/4 [00:09&lt;00:28,  9.67s/it]Stacking Bands...\nExtracting Bands:  50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:19&lt;00:19,  9.55s/it]Stacking Bands...\nExtracting Bands:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:29&lt;00:09,  9.78s/it]Stacking Bands...\nExtracting Bands: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:37&lt;00:00,  9.28s/it]\n</pre> In\u00a0[18]: Copied! <pre>\n</pre>"},{"location":"examples/usage/#install","title":"Install\u00b6","text":"<p>Installation is easy just grab it form pip</p>"},{"location":"examples/usage/#setup","title":"Setup\u00b6","text":"<p>Define the parameters for your computation</p> <p>Use this for visual image timeseries</p> <pre>formula=\"band1\"\nband1=\"visual\"\nband2=None\noperation=None\ntimeseries=True\n</pre>"},{"location":"examples/usage/#cleanup","title":"Cleanup\u00b6","text":"<p>Lets clear if there are previous output in the dir</p>"},{"location":"examples/usage/#example-compute-on-time-dimension","title":"Example compute on time dimension\u00b6","text":"<p>You can run computation now , You can visualize the results or download them from the files directly after computation is done</p>"},{"location":"examples/usage/#example-on-the-fly-tile-computation","title":"Example on the fly tile computation\u00b6","text":"<p>Here computation will be done alike google earth engine , you zoom out do computation , zoom in do computation based on your formula and resolution will differ accordingly as resampling will happen</p>"},{"location":"examples/usage/#extract-sentinel-2-images-for-your-aoi","title":"Extract Sentinel-2 Images for your AOI\u00b6","text":"<p>This example showcase how you can select the band you want and extract sentinel-2 images just for your area of interest</p>"},{"location":"src/engine/","title":"Engine Module","text":""},{"location":"src/engine/#virtughan.engine","title":"<code>virtughan.engine</code>","text":""},{"location":"src/engine/#virtughan.engine.bbox","title":"<code>bbox = [83.84765625, 28.22697003891833, 83.935546875, 28.304380682962773]</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.start_date","title":"<code>start_date = '2024-12-15'</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.end_date","title":"<code>end_date = '2024-12-31'</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.cloud_cover","title":"<code>cloud_cover = 30</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.formula","title":"<code>formula = '(band2-band1)/(band2+band1)'</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.band1","title":"<code>band1 = 'red'</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.band2","title":"<code>band2 = 'nir'</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.operation","title":"<code>operation = 'median'</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.timeseries","title":"<code>timeseries = True</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.output_dir","title":"<code>output_dir = './output'</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.workers","title":"<code>workers = 1</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.processor","title":"<code>processor = VirtughanProcessor(bbox, start_date, end_date, cloud_cover, formula, band1, band2, operation, timeseries, output_dir, workers=workers)</code>  <code>module-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor","title":"<code>VirtughanProcessor</code>","text":"<p>Processor for virtual computation cubes.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>class VirtughanProcessor:\n    \"\"\"\n    Processor for virtual computation cubes.\n    \"\"\"\n\n    def __init__(\n        self,\n        bbox,\n        start_date,\n        end_date,\n        cloud_cover,\n        formula,\n        band1,\n        band2,\n        operation,\n        timeseries,\n        output_dir,\n        log_file=sys.stdout,\n        cmap=\"RdYlGn\",\n        workers=1,\n        smart_filter=True,\n    ):\n        \"\"\"\n        Initialize the VirtughanProcessor.\n\n        Parameters:\n        bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat].\n        start_date (str): Start date for the data extraction (YYYY-MM-DD).\n        end_date (str): End date for the data extraction (YYYY-MM-DD).\n        cloud_cover (int): Maximum allowed cloud cover percentage.\n        formula (str): Formula to apply to the bands.\n        band1 (str): First band for the formula.\n        band2 (str): Second band for the formula.\n        operation (str): Operation to apply to the time series.\n        timeseries (bool): Whether to generate a time series.\n        output_dir (str): Directory to save the output files.\n        log_file (file): File to log the processing.\n        cmap (str): Colormap to apply to the results.\n        workers (int): Number of parallel workers.\n        smart_filter (bool): Whether to apply smart filtering to the images.\n        \"\"\"\n        self.bbox = bbox\n        self.start_date = start_date\n        self.end_date = end_date\n        self.cloud_cover = cloud_cover\n        self.formula = formula or \"band1\"\n        self.band1 = band1\n        self.band2 = band2\n        self.operation = operation\n        self.timeseries = timeseries\n        self.output_dir = output_dir\n        self.log_file = log_file\n        self.cmap = cmap\n        self.workers = workers\n        self.result_list = []\n        self.dates = []\n        self.crs = None\n        self.transform = None\n        self.intermediate_images = []\n        self.intermediate_images_with_text = []\n        self.use_smart_filter = smart_filter\n\n    def fetch_process_custom_band(self, band1_url, band2_url):\n        \"\"\"\n        Fetch and process custom band data, resampling if bands have different resolutions.\n\n        Parameters:\n        band1_url (str): URL of the first band.\n        band2_url (str): URL of the second band.\n\n        Returns:\n        tuple: Processed result, CRS, transform, and band URL.\n        \"\"\"\n\n        try:\n            with rio.open(band1_url) as band1_cog:\n                min_x, min_y, max_x, max_y = self._transform_bbox(band1_cog.crs)\n                band1_window = self._calculate_window(\n                    band1_cog, min_x, min_y, max_x, max_y\n                )\n\n                if self._is_window_out_of_bounds(band1_window):\n                    return None, None, None, None\n\n                band1_data = band1_cog.read(window=band1_window).astype(float)\n                band1_transform = band1_cog.window_transform(band1_window)\n                band1_height, band1_width = band1_data.shape[1], band1_data.shape[2]\n\n                if band2_url:\n                    with rio.open(band2_url) as band2_cog:\n                        min_x, min_y, max_x, max_y = self._transform_bbox(band2_cog.crs)\n                        band2_window = self._calculate_window(\n                            band2_cog, min_x, min_y, max_x, max_y\n                        )\n\n                        if self._is_window_out_of_bounds(band2_window):\n                            return None, None, None, None\n\n                        band2_data = band2_cog.read(window=band2_window).astype(float)\n                        band2_transform = band2_cog.window_transform(band2_window)\n                        band2_height, band2_width = (\n                            band2_data.shape[1],\n                            band2_data.shape[2],\n                        )\n\n                        if band1_height != band2_height or band1_width != band2_width:\n                            band1_res = band1_transform[0]\n                            band2_res = band2_transform[0]\n\n                            if band1_res &gt; band2_res:\n                                resampled_band2 = np.zeros_like(band1_data)\n\n                                resampled_band2, _ = reproject(\n                                    source=band2_data,\n                                    destination=resampled_band2,\n                                    src_transform=band2_transform,\n                                    src_crs=band2_cog.crs,\n                                    dst_transform=band1_transform,\n                                    dst_crs=band1_cog.crs,\n                                    resampling=rio.warp.Resampling.bilinear,\n                                    dst_shape=(band1_height, band1_width),\n                                )\n                                band2_data = resampled_band2\n                                transform = band1_transform\n                            else:\n                                resampled_band1 = np.zeros_like(band2_data)\n\n                                resampled_band1, _ = reproject(\n                                    source=band1_data,\n                                    destination=resampled_band1,\n                                    src_transform=band1_transform,\n                                    src_crs=band1_cog.crs,\n                                    dst_transform=band2_transform,\n                                    dst_crs=band2_cog.crs,\n                                    resampling=rio.warp.Resampling.bilinear,\n                                    dst_shape=(band2_height, band2_width),\n                                )\n                                band1_data = resampled_band1\n                                transform = band2_transform\n                        else:\n                            transform = band1_transform\n\n                        band1 = band1_data\n                        band2 = band2_data\n                        result = eval(self.formula)\n                else:\n                    result = (\n                        eval(self.formula) if band1_data.shape[0] == 1 else band1_data\n                    )\n                    transform = band1_transform\n\n            return result, band1_cog.crs, transform, band1_url\n\n        except Exception as e:\n            raise e\n            print(f\"Error fetching image: {e}\")\n            return None, None, None, None\n\n    def _remove_overlapping_sentinel2_tiles(self, features):\n        \"\"\"\n        Remove overlapping Sentinel-2 tiles.\n\n        Parameters:\n        features (list): List of features to process.\n\n        Returns:\n        list: List of non-overlapping features.\n        \"\"\"\n        zone_counts = {}\n        # lets see how many zones we have in total images\n        for feature in features:\n            zone = feature[\"id\"].split(\"_\")[1][:2]\n            zone_counts[zone] = zone_counts.get(zone, 0) + 1\n        # lets get the maximum occorance zone so that when we remove duplicates later on we atleast will try to keep the same zone tiles\n        max_zone = max(zone_counts, key=zone_counts.get)\n\n        filtered_features = {}\n        for feature in features:\n            parts = feature[\"id\"].split(\"_\")\n            date = parts[2]\n            zone = parts[1][:2]\n\n            # if the zone is the most occuring zone then we will keep it but making sure that same date image is not present in the filtered list\n            if zone == max_zone and date not in filtered_features:\n                filtered_features[date] = feature\n\n        return list(filtered_features.values())\n\n    def _transform_bbox(self, crs):\n        \"\"\"\n        Transform the bounding box coordinates to the specified CRS.\n\n        Parameters:\n        crs (str): Coordinate reference system to transform to.\n\n        Returns:\n        tuple: Transformed bounding box coordinates (min_x, min_y, max_x, max_y).\n        \"\"\"\n        transformer = Transformer.from_crs(\"epsg:4326\", crs, always_xy=True)\n        min_x, min_y = transformer.transform(self.bbox[0], self.bbox[1])\n        max_x, max_y = transformer.transform(self.bbox[2], self.bbox[3])\n        return min_x, min_y, max_x, max_y\n\n    def _calculate_window(self, cog, min_x, min_y, max_x, max_y):\n        \"\"\"\n        Calculate the window for reading the data from the COG.\n\n        Parameters:\n        cog (rasterio.io.DatasetReader): COG dataset reader.\n        min_x (float): Minimum x-coordinate.\n        min_y (float): Minimum y-coordinate.\n        max_x (float): Maximum x-coordinate.\n        max_y (float): Maximum y-coordinate.\n\n        Returns:\n        rasterio.windows.Window: Window for reading the data.\n        \"\"\"\n        return from_bounds(min_x, min_y, max_x, max_y, cog.transform)\n\n    def _is_window_out_of_bounds(self, window):\n        \"\"\"\n        Check if the window is out of bounds.\n\n        Parameters:\n        window (rasterio.windows.Window): Window to check.\n\n        Returns:\n        bool: True if the window is out of bounds, False otherwise.\n        \"\"\"\n        return (\n            window.col_off &lt; 0\n            or window.row_off &lt; 0\n            or window.width &lt;= 0\n            or window.height &lt;= 0\n        )\n\n    def _get_band_urls(self, features):\n        \"\"\"\n        Get the URLs of the bands to be processed.\n\n        Parameters:\n        features (list): List of features containing the band URLs.\n\n        Returns:\n        tuple: List of URLs for band1 and band2.\n        \"\"\"\n        band1_urls = [feature[\"assets\"][self.band1][\"href\"] for feature in features]\n        band2_urls = (\n            [feature[\"assets\"][self.band2][\"href\"] for feature in features]\n            if self.band2\n            else [None] * len(features)\n        )\n        return band1_urls, band2_urls\n\n    def _process_images(self):\n        \"\"\"\n        Process the images and compute the results.\n        \"\"\"\n        features = search_stac_api(\n            self.bbox,\n            self.start_date,\n            self.end_date,\n            self.cloud_cover,\n        )\n        print(f\"Total scenes found: {len(features)}\")\n        filtered_features = filter_intersected_features(features, self.bbox)\n        print(f\"Scenes covering input area: {len(filtered_features)}\")\n        overlapping_features_removed = remove_overlapping_sentinel2_tiles(\n            filtered_features\n        )\n        print(f\"Scenes after removing overlaps: {len(overlapping_features_removed)}\")\n        if self.use_smart_filter:\n            overlapping_features_removed = smart_filter_images(\n                overlapping_features_removed, self.start_date, self.end_date\n            )\n            print(\n                f\"Scenes after applying smart filter: {len(overlapping_features_removed)}\"\n            )\n\n        band1_urls, band2_urls = self._get_band_urls(overlapping_features_removed)\n\n        if self.workers &gt; 1:\n            print(\"Using Parallel Processing...\")\n            with ThreadPoolExecutor(max_workers=self.workers) as executor:\n                futures = [\n                    executor.submit(\n                        self.fetch_process_custom_band, band1_url, band2_url\n                    )\n                    for band1_url, band2_url in zip(band1_urls, band2_urls)\n                ]\n                for future in tqdm(\n                    as_completed(futures),\n                    total=len(futures),\n                    desc=\"Computing Band Calculation\",\n                    file=self.log_file,\n                ):\n                    result, crs, transform, name_url = future.result()\n                    if result is not None:\n                        self.result_list.append(result)\n                        self.crs = crs\n                        self.transform = transform\n                        parts = name_url.split(\"/\")\n                        image_name = parts[\n                            -2\n                        ]  # fix this for other images than sentinel\n                        self.dates.append(image_name.split(\"_\")[2])\n                        if self.timeseries:\n                            self._save_intermediate_image(result, image_name)\n        else:\n            for band1_url, band2_url in tqdm(\n                zip(band1_urls, band2_urls),\n                total=len(band1_urls),\n                desc=\"Computing Band Calculation\",\n                file=self.log_file,\n            ):\n                result, self.crs, self.transform, name_url = (\n                    self.fetch_process_custom_band(band1_url, band2_url)\n                )\n                if result is not None:\n                    self.result_list.append(result)\n                    parts = name_url.split(\"/\")\n                    image_name = parts[-2]\n                    self.dates.append(image_name.split(\"_\")[2])\n\n                    if self.timeseries:\n                        self._save_intermediate_image(result, image_name)\n\n    def _save_intermediate_image(self, result, image_name):\n        \"\"\"\n        Save an intermediate image.\n\n        Parameters:\n        result (numpy.ndarray): Array of the result to save.\n        image_name (str): Name of the image file.\n        \"\"\"\n        output_file = os.path.join(self.output_dir, f\"{image_name}_result.tif\")\n        self._save_geotiff(result, output_file)\n        self.intermediate_images.append(output_file)\n        self.intermediate_images_with_text.append(\n            self.add_text_to_image(output_file, image_name)\n        )\n\n    def _save_geotiff(self, data, output_file):\n        \"\"\"\n        Save the data as a GeoTIFF file.\n\n        Parameters:\n        data (numpy.ndarray): Array of data to save.\n        output_file (str): Path to the output file.\n        \"\"\"\n        nodata_value = -9999\n        data = np.where(np.isnan(data), nodata_value, data)\n\n        with rio.open(\n            output_file,\n            \"w\",\n            driver=\"GTiff\",\n            height=data.shape[1],\n            width=data.shape[2],\n            count=data.shape[0],\n            dtype=data.dtype,\n            crs=self.crs,\n            transform=self.transform,\n            nodata=nodata_value,\n        ) as dst:\n            for band in range(1, data.shape[0] + 1):\n                dst.write(data[band - 1], band)\n\n    def _aggregate_results(self):\n        \"\"\"\n        Aggregate the results over time.\n\n        Returns:\n        numpy.ndarray: Aggregated result.\n        \"\"\"\n        sorted_dates_and_results = sorted(\n            zip(self.dates, self.result_list), key=lambda x: x[0]\n        )\n        sorted_dates, sorted_results = zip(*sorted_dates_and_results)\n\n        max_shape = tuple(max(s) for s in zip(*[arr.shape for arr in sorted_results]))\n        padded_result_list = [self._pad_array(arr, max_shape) for arr in sorted_results]\n        result_stack = np.ma.stack(padded_result_list)\n\n        operations = {\n            \"mean\": np.ma.mean,\n            \"median\": np.ma.median,\n            \"max\": np.ma.max,\n            \"min\": np.ma.min,\n            \"std\": np.ma.std,\n            \"sum\": np.ma.sum,\n            \"var\": np.ma.var,\n        }\n\n        aggregated_result = operations[self.operation](result_stack, axis=0)\n\n        dates = sorted_dates\n        dates_numeric = np.arange(len(dates))\n\n        values_per_date = operations[self.operation](result_stack, axis=(1, 2, 3))\n\n        slope, intercept = np.polyfit(dates_numeric, values_per_date, 1)\n        trend_line = slope * dates_numeric + intercept\n\n        plt.figure(figsize=(10, 5))\n        plt.plot(\n            dates,\n            values_per_date,\n            marker=\"o\",\n            linestyle=\"-\",\n            label=f\"{self.operation.capitalize()} Value\",\n        )\n        plt.plot(dates, trend_line, color=\"red\", linestyle=\"--\", label=\"Trend Line\")\n        plt.xlabel(\"Date\")\n        plt.ylabel(f\"{self.operation.capitalize()} Value\")\n        plt.title(f\"{self.operation.capitalize()} Value Over Time\")\n        plt.grid(True)\n        plt.xticks(rotation=45)\n        plt.legend()\n        plt.tight_layout()\n\n        plt.savefig(os.path.join(self.output_dir, \"values_over_time.png\"))\n        plt.close()\n\n        return aggregated_result\n\n    def save_aggregated_result_with_colormap(self, result_aggregate, output_file):\n        \"\"\"\n        Save the aggregated result with a colormap.\n\n        Parameters:\n        result_aggregate (numpy.ndarray): Aggregated result to save.\n        output_file (str): Path to the output file.\n        \"\"\"\n        result_aggregate = np.ma.masked_invalid(result_aggregate)\n        image = self._create_image(result_aggregate)\n        self._plot_result(image, output_file)\n        self._save_geotiff(result_aggregate, output_file)\n\n    def _create_image(self, data):\n        \"\"\"\n        Create an image from the data.\n\n        Parameters:\n        data (numpy.ndarray): Array of data to create the image from.\n\n        Returns:\n        numpy.ndarray: Image array.\n        \"\"\"\n        if data.shape[0] == 1:\n            result_normalized = (data[0] - data[0].min()) / (\n                data[0].max() - data[0].min()\n            )\n            colormap = plt.get_cmap(self.cmap)\n            result_colored = colormap(result_normalized)\n            return (result_colored[:, :, :3] * 255).astype(np.uint8)\n        else:\n            image_array = np.transpose(data, (1, 2, 0))\n            image_array = (\n                (image_array - image_array.min())\n                / (image_array.max() - image_array.min())\n                * 255\n            )\n            return image_array.astype(np.uint8)\n\n    def _plot_result(self, image, output_file):\n        \"\"\"\n        Plot the result and save it as an image.\n\n        Parameters:\n        image (numpy.ndarray): Image array to plot.\n        output_file (str): Path to the output file.\n        \"\"\"\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image)\n        plt.title(f\"Aggregated {self.operation} Calculation\")\n        plt.xlabel(\n            f\"From {self.start_date} to {self.end_date}\\nCloud Cover &lt; {self.cloud_cover}%\\nBBox: {self.bbox}\\nTotal Scene Processed: {len(self.result_list)}\"\n        )\n        plt.colorbar(\n            plt.cm.ScalarMappable(\n                cmap=plt.get_cmap(self.cmap),\n            ),\n            ax=plt.gca(),\n            shrink=0.5,\n        )\n        plt.savefig(\n            output_file.replace(\".tif\", \"_colormap.png\"),\n            bbox_inches=\"tight\",\n            pad_inches=0.1,\n        )\n        plt.close()\n\n    def _pad_array(self, array, target_shape, fill_value=np.nan):\n        \"\"\"\n        Pad the array to the target shape.\n\n        Parameters:\n        array (numpy.ndarray): Array to pad.\n        target_shape (tuple): Target shape to pad to.\n        fill_value (float): Value to use for padding.\n\n        Returns:\n        numpy.ndarray: Padded array.\n        \"\"\"\n        pad_width = [\n            (0, max(0, target - current))\n            for current, target in zip(array.shape, target_shape)\n        ]\n        return np.pad(array, pad_width, mode=\"constant\", constant_values=fill_value)\n\n    def add_text_to_image(self, image_path, text):\n        \"\"\"\n        Add text to an image.\n\n        Parameters:\n        image_path (str): Path to the image file.\n        text (str): Text to add to the image.\n\n        Returns:\n        str: Path to the image file with the added text.\n        \"\"\"\n        with rio.open(image_path) as src:\n            image_array = (\n                src.read(1)\n                if src.count == 1\n                else np.dstack([src.read(i) for i in range(1, 4)])\n            )\n            image_array = (\n                (image_array - image_array.min())\n                / (image_array.max() - image_array.min())\n                * 255\n            )\n            image = Image.fromarray(image_array.astype(np.uint8))\n\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image, cmap=self.cmap if src.count == 1 else None)\n        plt.axis(\"off\")\n        plt.title(text)\n        temp_image_path = os.path.splitext(image_path)[0] + \"_text.png\"\n        plt.savefig(temp_image_path, bbox_inches=\"tight\", pad_inches=0.1)\n        plt.close()\n        return temp_image_path\n\n    @staticmethod\n    def create_gif(image_list, output_path, duration_per_image=1):\n        \"\"\"\n        Create a GIF from a list of images.\n\n        Parameters:\n        image_list (list): List of image file paths.\n        output_path (str): Path to the output GIF file.\n        duration_per_image (int): Duration per image in the GIF (seconds).\n        \"\"\"\n        sorted_image_list = sorted(image_list)\n\n        images = [Image.open(image_path) for image_path in sorted_image_list]\n        max_width = max(image.width for image in images)\n        max_height = max(image.height for image in images)\n        resized_images = [\n            image.resize((max_width, max_height), Image.LANCZOS) for image in images\n        ]\n\n        frame_duration = duration_per_image * 1000\n\n        resized_images[0].save(\n            output_path,\n            save_all=True,\n            append_images=resized_images[1:],\n            duration=frame_duration,\n            loop=0,\n        )\n        print(f\"Saved timeseries GIF to {output_path}\")\n\n    def compute(self):\n        \"\"\"\n        Compute the results based on the provided parameters.\n        \"\"\"\n        print(\"Engine starting...\")\n        os.makedirs(self.output_dir, exist_ok=True)\n        if not self.band1:\n            raise Exception(\"Band1 is required\")\n\n        print(\"Searching STAC .....\")\n        self._process_images()\n\n        if self.result_list and self.operation:\n            print(\"Aggregating results...\")\n            result_aggregate = self._aggregate_results()\n            output_file = os.path.join(\n                self.output_dir, \"custom_band_output_aggregate.tif\"\n            )\n            print(\"Saving aggregated result with colormap...\")\n            self.save_aggregated_result_with_colormap(result_aggregate, output_file)\n\n        if self.timeseries:\n            print(\"Creating GIF and zipping TIFF files...\")\n            if self.intermediate_images:\n                self.create_gif(\n                    self.intermediate_images_with_text,\n                    os.path.join(self.output_dir, \"output.gif\"),\n                )\n                zip_files(\n                    self.intermediate_images,\n                    os.path.join(self.output_dir, \"tiff_files.zip\"),\n                )\n            else:\n                print(\"No images found for the given parameters\")\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.bbox","title":"<code>bbox = bbox</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.start_date","title":"<code>start_date = start_date</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.end_date","title":"<code>end_date = end_date</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.cloud_cover","title":"<code>cloud_cover = cloud_cover</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.formula","title":"<code>formula = formula or 'band1'</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.band1","title":"<code>band1 = band1</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.band2","title":"<code>band2 = band2</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.operation","title":"<code>operation = operation</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.timeseries","title":"<code>timeseries = timeseries</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.output_dir","title":"<code>output_dir = output_dir</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.log_file","title":"<code>log_file = log_file</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.cmap","title":"<code>cmap = cmap</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.workers","title":"<code>workers = workers</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.result_list","title":"<code>result_list = []</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.dates","title":"<code>dates = []</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.crs","title":"<code>crs = None</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.transform","title":"<code>transform = None</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.intermediate_images","title":"<code>intermediate_images = []</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.intermediate_images_with_text","title":"<code>intermediate_images_with_text = []</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.use_smart_filter","title":"<code>use_smart_filter = smart_filter</code>  <code>instance-attribute</code>","text":""},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.__init__","title":"<code>__init__(bbox, start_date, end_date, cloud_cover, formula, band1, band2, operation, timeseries, output_dir, log_file=sys.stdout, cmap='RdYlGn', workers=1, smart_filter=True)</code>","text":"<p>Initialize the VirtughanProcessor.</p> <p>Parameters: bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat]. start_date (str): Start date for the data extraction (YYYY-MM-DD). end_date (str): End date for the data extraction (YYYY-MM-DD). cloud_cover (int): Maximum allowed cloud cover percentage. formula (str): Formula to apply to the bands. band1 (str): First band for the formula. band2 (str): Second band for the formula. operation (str): Operation to apply to the time series. timeseries (bool): Whether to generate a time series. output_dir (str): Directory to save the output files. log_file (file): File to log the processing. cmap (str): Colormap to apply to the results. workers (int): Number of parallel workers. smart_filter (bool): Whether to apply smart filtering to the images.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def __init__(\n    self,\n    bbox,\n    start_date,\n    end_date,\n    cloud_cover,\n    formula,\n    band1,\n    band2,\n    operation,\n    timeseries,\n    output_dir,\n    log_file=sys.stdout,\n    cmap=\"RdYlGn\",\n    workers=1,\n    smart_filter=True,\n):\n    \"\"\"\n    Initialize the VirtughanProcessor.\n\n    Parameters:\n    bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat].\n    start_date (str): Start date for the data extraction (YYYY-MM-DD).\n    end_date (str): End date for the data extraction (YYYY-MM-DD).\n    cloud_cover (int): Maximum allowed cloud cover percentage.\n    formula (str): Formula to apply to the bands.\n    band1 (str): First band for the formula.\n    band2 (str): Second band for the formula.\n    operation (str): Operation to apply to the time series.\n    timeseries (bool): Whether to generate a time series.\n    output_dir (str): Directory to save the output files.\n    log_file (file): File to log the processing.\n    cmap (str): Colormap to apply to the results.\n    workers (int): Number of parallel workers.\n    smart_filter (bool): Whether to apply smart filtering to the images.\n    \"\"\"\n    self.bbox = bbox\n    self.start_date = start_date\n    self.end_date = end_date\n    self.cloud_cover = cloud_cover\n    self.formula = formula or \"band1\"\n    self.band1 = band1\n    self.band2 = band2\n    self.operation = operation\n    self.timeseries = timeseries\n    self.output_dir = output_dir\n    self.log_file = log_file\n    self.cmap = cmap\n    self.workers = workers\n    self.result_list = []\n    self.dates = []\n    self.crs = None\n    self.transform = None\n    self.intermediate_images = []\n    self.intermediate_images_with_text = []\n    self.use_smart_filter = smart_filter\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.fetch_process_custom_band","title":"<code>fetch_process_custom_band(band1_url, band2_url)</code>","text":"<p>Fetch and process custom band data, resampling if bands have different resolutions.</p> <p>Parameters: band1_url (str): URL of the first band. band2_url (str): URL of the second band.</p> <p>Returns: tuple: Processed result, CRS, transform, and band URL.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def fetch_process_custom_band(self, band1_url, band2_url):\n    \"\"\"\n    Fetch and process custom band data, resampling if bands have different resolutions.\n\n    Parameters:\n    band1_url (str): URL of the first band.\n    band2_url (str): URL of the second band.\n\n    Returns:\n    tuple: Processed result, CRS, transform, and band URL.\n    \"\"\"\n\n    try:\n        with rio.open(band1_url) as band1_cog:\n            min_x, min_y, max_x, max_y = self._transform_bbox(band1_cog.crs)\n            band1_window = self._calculate_window(\n                band1_cog, min_x, min_y, max_x, max_y\n            )\n\n            if self._is_window_out_of_bounds(band1_window):\n                return None, None, None, None\n\n            band1_data = band1_cog.read(window=band1_window).astype(float)\n            band1_transform = band1_cog.window_transform(band1_window)\n            band1_height, band1_width = band1_data.shape[1], band1_data.shape[2]\n\n            if band2_url:\n                with rio.open(band2_url) as band2_cog:\n                    min_x, min_y, max_x, max_y = self._transform_bbox(band2_cog.crs)\n                    band2_window = self._calculate_window(\n                        band2_cog, min_x, min_y, max_x, max_y\n                    )\n\n                    if self._is_window_out_of_bounds(band2_window):\n                        return None, None, None, None\n\n                    band2_data = band2_cog.read(window=band2_window).astype(float)\n                    band2_transform = band2_cog.window_transform(band2_window)\n                    band2_height, band2_width = (\n                        band2_data.shape[1],\n                        band2_data.shape[2],\n                    )\n\n                    if band1_height != band2_height or band1_width != band2_width:\n                        band1_res = band1_transform[0]\n                        band2_res = band2_transform[0]\n\n                        if band1_res &gt; band2_res:\n                            resampled_band2 = np.zeros_like(band1_data)\n\n                            resampled_band2, _ = reproject(\n                                source=band2_data,\n                                destination=resampled_band2,\n                                src_transform=band2_transform,\n                                src_crs=band2_cog.crs,\n                                dst_transform=band1_transform,\n                                dst_crs=band1_cog.crs,\n                                resampling=rio.warp.Resampling.bilinear,\n                                dst_shape=(band1_height, band1_width),\n                            )\n                            band2_data = resampled_band2\n                            transform = band1_transform\n                        else:\n                            resampled_band1 = np.zeros_like(band2_data)\n\n                            resampled_band1, _ = reproject(\n                                source=band1_data,\n                                destination=resampled_band1,\n                                src_transform=band1_transform,\n                                src_crs=band1_cog.crs,\n                                dst_transform=band2_transform,\n                                dst_crs=band2_cog.crs,\n                                resampling=rio.warp.Resampling.bilinear,\n                                dst_shape=(band2_height, band2_width),\n                            )\n                            band1_data = resampled_band1\n                            transform = band2_transform\n                    else:\n                        transform = band1_transform\n\n                    band1 = band1_data\n                    band2 = band2_data\n                    result = eval(self.formula)\n            else:\n                result = (\n                    eval(self.formula) if band1_data.shape[0] == 1 else band1_data\n                )\n                transform = band1_transform\n\n        return result, band1_cog.crs, transform, band1_url\n\n    except Exception as e:\n        raise e\n        print(f\"Error fetching image: {e}\")\n        return None, None, None, None\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._remove_overlapping_sentinel2_tiles","title":"<code>_remove_overlapping_sentinel2_tiles(features)</code>","text":"<p>Remove overlapping Sentinel-2 tiles.</p> <p>Parameters: features (list): List of features to process.</p> <p>Returns: list: List of non-overlapping features.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _remove_overlapping_sentinel2_tiles(self, features):\n    \"\"\"\n    Remove overlapping Sentinel-2 tiles.\n\n    Parameters:\n    features (list): List of features to process.\n\n    Returns:\n    list: List of non-overlapping features.\n    \"\"\"\n    zone_counts = {}\n    # lets see how many zones we have in total images\n    for feature in features:\n        zone = feature[\"id\"].split(\"_\")[1][:2]\n        zone_counts[zone] = zone_counts.get(zone, 0) + 1\n    # lets get the maximum occorance zone so that when we remove duplicates later on we atleast will try to keep the same zone tiles\n    max_zone = max(zone_counts, key=zone_counts.get)\n\n    filtered_features = {}\n    for feature in features:\n        parts = feature[\"id\"].split(\"_\")\n        date = parts[2]\n        zone = parts[1][:2]\n\n        # if the zone is the most occuring zone then we will keep it but making sure that same date image is not present in the filtered list\n        if zone == max_zone and date not in filtered_features:\n            filtered_features[date] = feature\n\n    return list(filtered_features.values())\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._transform_bbox","title":"<code>_transform_bbox(crs)</code>","text":"<p>Transform the bounding box coordinates to the specified CRS.</p> <p>Parameters: crs (str): Coordinate reference system to transform to.</p> <p>Returns: tuple: Transformed bounding box coordinates (min_x, min_y, max_x, max_y).</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _transform_bbox(self, crs):\n    \"\"\"\n    Transform the bounding box coordinates to the specified CRS.\n\n    Parameters:\n    crs (str): Coordinate reference system to transform to.\n\n    Returns:\n    tuple: Transformed bounding box coordinates (min_x, min_y, max_x, max_y).\n    \"\"\"\n    transformer = Transformer.from_crs(\"epsg:4326\", crs, always_xy=True)\n    min_x, min_y = transformer.transform(self.bbox[0], self.bbox[1])\n    max_x, max_y = transformer.transform(self.bbox[2], self.bbox[3])\n    return min_x, min_y, max_x, max_y\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._calculate_window","title":"<code>_calculate_window(cog, min_x, min_y, max_x, max_y)</code>","text":"<p>Calculate the window for reading the data from the COG.</p> <p>Parameters: cog (rasterio.io.DatasetReader): COG dataset reader. min_x (float): Minimum x-coordinate. min_y (float): Minimum y-coordinate. max_x (float): Maximum x-coordinate. max_y (float): Maximum y-coordinate.</p> <p>Returns: rasterio.windows.Window: Window for reading the data.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _calculate_window(self, cog, min_x, min_y, max_x, max_y):\n    \"\"\"\n    Calculate the window for reading the data from the COG.\n\n    Parameters:\n    cog (rasterio.io.DatasetReader): COG dataset reader.\n    min_x (float): Minimum x-coordinate.\n    min_y (float): Minimum y-coordinate.\n    max_x (float): Maximum x-coordinate.\n    max_y (float): Maximum y-coordinate.\n\n    Returns:\n    rasterio.windows.Window: Window for reading the data.\n    \"\"\"\n    return from_bounds(min_x, min_y, max_x, max_y, cog.transform)\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._is_window_out_of_bounds","title":"<code>_is_window_out_of_bounds(window)</code>","text":"<p>Check if the window is out of bounds.</p> <p>Parameters: window (rasterio.windows.Window): Window to check.</p> <p>Returns: bool: True if the window is out of bounds, False otherwise.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _is_window_out_of_bounds(self, window):\n    \"\"\"\n    Check if the window is out of bounds.\n\n    Parameters:\n    window (rasterio.windows.Window): Window to check.\n\n    Returns:\n    bool: True if the window is out of bounds, False otherwise.\n    \"\"\"\n    return (\n        window.col_off &lt; 0\n        or window.row_off &lt; 0\n        or window.width &lt;= 0\n        or window.height &lt;= 0\n    )\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._get_band_urls","title":"<code>_get_band_urls(features)</code>","text":"<p>Get the URLs of the bands to be processed.</p> <p>Parameters: features (list): List of features containing the band URLs.</p> <p>Returns: tuple: List of URLs for band1 and band2.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _get_band_urls(self, features):\n    \"\"\"\n    Get the URLs of the bands to be processed.\n\n    Parameters:\n    features (list): List of features containing the band URLs.\n\n    Returns:\n    tuple: List of URLs for band1 and band2.\n    \"\"\"\n    band1_urls = [feature[\"assets\"][self.band1][\"href\"] for feature in features]\n    band2_urls = (\n        [feature[\"assets\"][self.band2][\"href\"] for feature in features]\n        if self.band2\n        else [None] * len(features)\n    )\n    return band1_urls, band2_urls\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._process_images","title":"<code>_process_images()</code>","text":"<p>Process the images and compute the results.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _process_images(self):\n    \"\"\"\n    Process the images and compute the results.\n    \"\"\"\n    features = search_stac_api(\n        self.bbox,\n        self.start_date,\n        self.end_date,\n        self.cloud_cover,\n    )\n    print(f\"Total scenes found: {len(features)}\")\n    filtered_features = filter_intersected_features(features, self.bbox)\n    print(f\"Scenes covering input area: {len(filtered_features)}\")\n    overlapping_features_removed = remove_overlapping_sentinel2_tiles(\n        filtered_features\n    )\n    print(f\"Scenes after removing overlaps: {len(overlapping_features_removed)}\")\n    if self.use_smart_filter:\n        overlapping_features_removed = smart_filter_images(\n            overlapping_features_removed, self.start_date, self.end_date\n        )\n        print(\n            f\"Scenes after applying smart filter: {len(overlapping_features_removed)}\"\n        )\n\n    band1_urls, band2_urls = self._get_band_urls(overlapping_features_removed)\n\n    if self.workers &gt; 1:\n        print(\"Using Parallel Processing...\")\n        with ThreadPoolExecutor(max_workers=self.workers) as executor:\n            futures = [\n                executor.submit(\n                    self.fetch_process_custom_band, band1_url, band2_url\n                )\n                for band1_url, band2_url in zip(band1_urls, band2_urls)\n            ]\n            for future in tqdm(\n                as_completed(futures),\n                total=len(futures),\n                desc=\"Computing Band Calculation\",\n                file=self.log_file,\n            ):\n                result, crs, transform, name_url = future.result()\n                if result is not None:\n                    self.result_list.append(result)\n                    self.crs = crs\n                    self.transform = transform\n                    parts = name_url.split(\"/\")\n                    image_name = parts[\n                        -2\n                    ]  # fix this for other images than sentinel\n                    self.dates.append(image_name.split(\"_\")[2])\n                    if self.timeseries:\n                        self._save_intermediate_image(result, image_name)\n    else:\n        for band1_url, band2_url in tqdm(\n            zip(band1_urls, band2_urls),\n            total=len(band1_urls),\n            desc=\"Computing Band Calculation\",\n            file=self.log_file,\n        ):\n            result, self.crs, self.transform, name_url = (\n                self.fetch_process_custom_band(band1_url, band2_url)\n            )\n            if result is not None:\n                self.result_list.append(result)\n                parts = name_url.split(\"/\")\n                image_name = parts[-2]\n                self.dates.append(image_name.split(\"_\")[2])\n\n                if self.timeseries:\n                    self._save_intermediate_image(result, image_name)\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._save_intermediate_image","title":"<code>_save_intermediate_image(result, image_name)</code>","text":"<p>Save an intermediate image.</p> <p>Parameters: result (numpy.ndarray): Array of the result to save. image_name (str): Name of the image file.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _save_intermediate_image(self, result, image_name):\n    \"\"\"\n    Save an intermediate image.\n\n    Parameters:\n    result (numpy.ndarray): Array of the result to save.\n    image_name (str): Name of the image file.\n    \"\"\"\n    output_file = os.path.join(self.output_dir, f\"{image_name}_result.tif\")\n    self._save_geotiff(result, output_file)\n    self.intermediate_images.append(output_file)\n    self.intermediate_images_with_text.append(\n        self.add_text_to_image(output_file, image_name)\n    )\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._save_geotiff","title":"<code>_save_geotiff(data, output_file)</code>","text":"<p>Save the data as a GeoTIFF file.</p> <p>Parameters: data (numpy.ndarray): Array of data to save. output_file (str): Path to the output file.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _save_geotiff(self, data, output_file):\n    \"\"\"\n    Save the data as a GeoTIFF file.\n\n    Parameters:\n    data (numpy.ndarray): Array of data to save.\n    output_file (str): Path to the output file.\n    \"\"\"\n    nodata_value = -9999\n    data = np.where(np.isnan(data), nodata_value, data)\n\n    with rio.open(\n        output_file,\n        \"w\",\n        driver=\"GTiff\",\n        height=data.shape[1],\n        width=data.shape[2],\n        count=data.shape[0],\n        dtype=data.dtype,\n        crs=self.crs,\n        transform=self.transform,\n        nodata=nodata_value,\n    ) as dst:\n        for band in range(1, data.shape[0] + 1):\n            dst.write(data[band - 1], band)\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._aggregate_results","title":"<code>_aggregate_results()</code>","text":"<p>Aggregate the results over time.</p> <p>Returns: numpy.ndarray: Aggregated result.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _aggregate_results(self):\n    \"\"\"\n    Aggregate the results over time.\n\n    Returns:\n    numpy.ndarray: Aggregated result.\n    \"\"\"\n    sorted_dates_and_results = sorted(\n        zip(self.dates, self.result_list), key=lambda x: x[0]\n    )\n    sorted_dates, sorted_results = zip(*sorted_dates_and_results)\n\n    max_shape = tuple(max(s) for s in zip(*[arr.shape for arr in sorted_results]))\n    padded_result_list = [self._pad_array(arr, max_shape) for arr in sorted_results]\n    result_stack = np.ma.stack(padded_result_list)\n\n    operations = {\n        \"mean\": np.ma.mean,\n        \"median\": np.ma.median,\n        \"max\": np.ma.max,\n        \"min\": np.ma.min,\n        \"std\": np.ma.std,\n        \"sum\": np.ma.sum,\n        \"var\": np.ma.var,\n    }\n\n    aggregated_result = operations[self.operation](result_stack, axis=0)\n\n    dates = sorted_dates\n    dates_numeric = np.arange(len(dates))\n\n    values_per_date = operations[self.operation](result_stack, axis=(1, 2, 3))\n\n    slope, intercept = np.polyfit(dates_numeric, values_per_date, 1)\n    trend_line = slope * dates_numeric + intercept\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(\n        dates,\n        values_per_date,\n        marker=\"o\",\n        linestyle=\"-\",\n        label=f\"{self.operation.capitalize()} Value\",\n    )\n    plt.plot(dates, trend_line, color=\"red\", linestyle=\"--\", label=\"Trend Line\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(f\"{self.operation.capitalize()} Value\")\n    plt.title(f\"{self.operation.capitalize()} Value Over Time\")\n    plt.grid(True)\n    plt.xticks(rotation=45)\n    plt.legend()\n    plt.tight_layout()\n\n    plt.savefig(os.path.join(self.output_dir, \"values_over_time.png\"))\n    plt.close()\n\n    return aggregated_result\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.save_aggregated_result_with_colormap","title":"<code>save_aggregated_result_with_colormap(result_aggregate, output_file)</code>","text":"<p>Save the aggregated result with a colormap.</p> <p>Parameters: result_aggregate (numpy.ndarray): Aggregated result to save. output_file (str): Path to the output file.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def save_aggregated_result_with_colormap(self, result_aggregate, output_file):\n    \"\"\"\n    Save the aggregated result with a colormap.\n\n    Parameters:\n    result_aggregate (numpy.ndarray): Aggregated result to save.\n    output_file (str): Path to the output file.\n    \"\"\"\n    result_aggregate = np.ma.masked_invalid(result_aggregate)\n    image = self._create_image(result_aggregate)\n    self._plot_result(image, output_file)\n    self._save_geotiff(result_aggregate, output_file)\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._create_image","title":"<code>_create_image(data)</code>","text":"<p>Create an image from the data.</p> <p>Parameters: data (numpy.ndarray): Array of data to create the image from.</p> <p>Returns: numpy.ndarray: Image array.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _create_image(self, data):\n    \"\"\"\n    Create an image from the data.\n\n    Parameters:\n    data (numpy.ndarray): Array of data to create the image from.\n\n    Returns:\n    numpy.ndarray: Image array.\n    \"\"\"\n    if data.shape[0] == 1:\n        result_normalized = (data[0] - data[0].min()) / (\n            data[0].max() - data[0].min()\n        )\n        colormap = plt.get_cmap(self.cmap)\n        result_colored = colormap(result_normalized)\n        return (result_colored[:, :, :3] * 255).astype(np.uint8)\n    else:\n        image_array = np.transpose(data, (1, 2, 0))\n        image_array = (\n            (image_array - image_array.min())\n            / (image_array.max() - image_array.min())\n            * 255\n        )\n        return image_array.astype(np.uint8)\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._plot_result","title":"<code>_plot_result(image, output_file)</code>","text":"<p>Plot the result and save it as an image.</p> <p>Parameters: image (numpy.ndarray): Image array to plot. output_file (str): Path to the output file.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _plot_result(self, image, output_file):\n    \"\"\"\n    Plot the result and save it as an image.\n\n    Parameters:\n    image (numpy.ndarray): Image array to plot.\n    output_file (str): Path to the output file.\n    \"\"\"\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    plt.title(f\"Aggregated {self.operation} Calculation\")\n    plt.xlabel(\n        f\"From {self.start_date} to {self.end_date}\\nCloud Cover &lt; {self.cloud_cover}%\\nBBox: {self.bbox}\\nTotal Scene Processed: {len(self.result_list)}\"\n    )\n    plt.colorbar(\n        plt.cm.ScalarMappable(\n            cmap=plt.get_cmap(self.cmap),\n        ),\n        ax=plt.gca(),\n        shrink=0.5,\n    )\n    plt.savefig(\n        output_file.replace(\".tif\", \"_colormap.png\"),\n        bbox_inches=\"tight\",\n        pad_inches=0.1,\n    )\n    plt.close()\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor._pad_array","title":"<code>_pad_array(array, target_shape, fill_value=np.nan)</code>","text":"<p>Pad the array to the target shape.</p> <p>Parameters: array (numpy.ndarray): Array to pad. target_shape (tuple): Target shape to pad to. fill_value (float): Value to use for padding.</p> <p>Returns: numpy.ndarray: Padded array.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def _pad_array(self, array, target_shape, fill_value=np.nan):\n    \"\"\"\n    Pad the array to the target shape.\n\n    Parameters:\n    array (numpy.ndarray): Array to pad.\n    target_shape (tuple): Target shape to pad to.\n    fill_value (float): Value to use for padding.\n\n    Returns:\n    numpy.ndarray: Padded array.\n    \"\"\"\n    pad_width = [\n        (0, max(0, target - current))\n        for current, target in zip(array.shape, target_shape)\n    ]\n    return np.pad(array, pad_width, mode=\"constant\", constant_values=fill_value)\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.add_text_to_image","title":"<code>add_text_to_image(image_path, text)</code>","text":"<p>Add text to an image.</p> <p>Parameters: image_path (str): Path to the image file. text (str): Text to add to the image.</p> <p>Returns: str: Path to the image file with the added text.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def add_text_to_image(self, image_path, text):\n    \"\"\"\n    Add text to an image.\n\n    Parameters:\n    image_path (str): Path to the image file.\n    text (str): Text to add to the image.\n\n    Returns:\n    str: Path to the image file with the added text.\n    \"\"\"\n    with rio.open(image_path) as src:\n        image_array = (\n            src.read(1)\n            if src.count == 1\n            else np.dstack([src.read(i) for i in range(1, 4)])\n        )\n        image_array = (\n            (image_array - image_array.min())\n            / (image_array.max() - image_array.min())\n            * 255\n        )\n        image = Image.fromarray(image_array.astype(np.uint8))\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image, cmap=self.cmap if src.count == 1 else None)\n    plt.axis(\"off\")\n    plt.title(text)\n    temp_image_path = os.path.splitext(image_path)[0] + \"_text.png\"\n    plt.savefig(temp_image_path, bbox_inches=\"tight\", pad_inches=0.1)\n    plt.close()\n    return temp_image_path\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.create_gif","title":"<code>create_gif(image_list, output_path, duration_per_image=1)</code>  <code>staticmethod</code>","text":"<p>Create a GIF from a list of images.</p> <p>Parameters: image_list (list): List of image file paths. output_path (str): Path to the output GIF file. duration_per_image (int): Duration per image in the GIF (seconds).</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>@staticmethod\ndef create_gif(image_list, output_path, duration_per_image=1):\n    \"\"\"\n    Create a GIF from a list of images.\n\n    Parameters:\n    image_list (list): List of image file paths.\n    output_path (str): Path to the output GIF file.\n    duration_per_image (int): Duration per image in the GIF (seconds).\n    \"\"\"\n    sorted_image_list = sorted(image_list)\n\n    images = [Image.open(image_path) for image_path in sorted_image_list]\n    max_width = max(image.width for image in images)\n    max_height = max(image.height for image in images)\n    resized_images = [\n        image.resize((max_width, max_height), Image.LANCZOS) for image in images\n    ]\n\n    frame_duration = duration_per_image * 1000\n\n    resized_images[0].save(\n        output_path,\n        save_all=True,\n        append_images=resized_images[1:],\n        duration=frame_duration,\n        loop=0,\n    )\n    print(f\"Saved timeseries GIF to {output_path}\")\n</code></pre>"},{"location":"src/engine/#virtughan.engine.VirtughanProcessor.compute","title":"<code>compute()</code>","text":"<p>Compute the results based on the provided parameters.</p> Source code in <code>src/virtughan/engine.py</code> <pre><code>def compute(self):\n    \"\"\"\n    Compute the results based on the provided parameters.\n    \"\"\"\n    print(\"Engine starting...\")\n    os.makedirs(self.output_dir, exist_ok=True)\n    if not self.band1:\n        raise Exception(\"Band1 is required\")\n\n    print(\"Searching STAC .....\")\n    self._process_images()\n\n    if self.result_list and self.operation:\n        print(\"Aggregating results...\")\n        result_aggregate = self._aggregate_results()\n        output_file = os.path.join(\n            self.output_dir, \"custom_band_output_aggregate.tif\"\n        )\n        print(\"Saving aggregated result with colormap...\")\n        self.save_aggregated_result_with_colormap(result_aggregate, output_file)\n\n    if self.timeseries:\n        print(\"Creating GIF and zipping TIFF files...\")\n        if self.intermediate_images:\n            self.create_gif(\n                self.intermediate_images_with_text,\n                os.path.join(self.output_dir, \"output.gif\"),\n            )\n            zip_files(\n                self.intermediate_images,\n                os.path.join(self.output_dir, \"tiff_files.zip\"),\n            )\n        else:\n            print(\"No images found for the given parameters\")\n</code></pre>"},{"location":"src/extract/","title":"Extract Module","text":""},{"location":"src/extract/#virtughan.extract","title":"<code>virtughan.extract</code>","text":""},{"location":"src/extract/#virtughan.extract.VALID_BANDS","title":"<code>VALID_BANDS = {'red': 'Red - 10m', 'green': 'Green - 10m', 'blue': 'Blue - 10m', 'nir': 'NIR 1 - 10m', 'swir22': 'SWIR 2.2\u03bcm - 20m', 'rededge2': 'Red Edge 2 - 20m', 'rededge3': 'Red Edge 3 - 20m', 'rededge1': 'Red Edge 1 - 20m', 'swir16': 'SWIR 1.6\u03bcm - 20m', 'wvp': 'Water Vapour (WVP)', 'nir08': 'NIR 2 - 20m', 'aot': 'Aerosol optical thickness (AOT)', 'coastal': 'Coastal - 60m', 'nir09': 'NIR 3 - 60m'}</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.bbox","title":"<code>bbox = [83.84765625, 28.22697003891833, 83.935546875, 28.304380682962773]</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.start_date","title":"<code>start_date = '2024-12-15'</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.end_date","title":"<code>end_date = '2024-12-31'</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.cloud_cover","title":"<code>cloud_cover = 30</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.bands_list","title":"<code>bands_list = ['red', 'nir', 'green']</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.output_dir","title":"<code>output_dir = './extracted_bands'</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.workers","title":"<code>workers = 1</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.extractor","title":"<code>extractor = ExtractProcessor(bbox, start_date, end_date, cloud_cover, bands_list, output_dir, workers=workers)</code>  <code>module-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor","title":"<code>ExtractProcessor</code>","text":"<p>Processor for extracting and saving bands from satellite images.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>class ExtractProcessor:\n    \"\"\"\n    Processor for extracting and saving bands from satellite images.\n    \"\"\"\n\n    def __init__(\n        self,\n        bbox,\n        start_date,\n        end_date,\n        cloud_cover,\n        bands_list,\n        output_dir,\n        log_file=sys.stdout,\n        workers=1,\n        zip_output=False,\n        smart_filter=True,\n    ):\n        \"\"\"\n        Initialize the ExtractProcessor.\n\n        Parameters:\n        bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat].\n        start_date (str): Start date for the data extraction (YYYY-MM-DD).\n        end_date (str): End date for the data extraction (YYYY-MM-DD).\n        cloud_cover (int): Maximum allowed cloud cover percentage.\n        bands_list (list): List of bands to extract.\n        output_dir (str): Directory to save the extracted bands.\n        log_file (file): File to log the extraction process.\n        workers (int): Number of parallel workers.\n        zip_output (bool): Whether to zip the output files.\n        smart_filter (bool): Whether to apply smart filtering to the images.\n        \"\"\"\n        self.bbox = bbox\n        self.start_date = start_date\n        self.end_date = end_date\n        self.cloud_cover = cloud_cover\n        self.bands_list = bands_list\n        self.output_dir = output_dir\n        self.log_file = log_file\n        self.workers = workers\n        self.zip_output = zip_output\n        self.crs = None\n        self.transform = None\n        self.use_smart_filter = smart_filter\n\n        self._validate_bands_list()\n\n    def _validate_bands_list(self):\n        \"\"\"\n        Validate the list of bands to ensure they are valid.\n        \"\"\"\n        invalid_bands = [band for band in self.bands_list if band not in VALID_BANDS]\n        if invalid_bands:\n            raise ValueError(\n                f\"Invalid band names: {', '.join(invalid_bands)}. \"\n                f\"Band names should be one of: {', '.join(VALID_BANDS.keys())}\"\n            )\n\n    def _transform_bbox(self, crs):\n        \"\"\"\n        Transform the bounding box coordinates to the specified CRS.\n\n        Parameters:\n        crs (str): Coordinate reference system to transform to.\n\n        Returns:\n        tuple: Transformed bounding box coordinates (min_x, min_y, max_x, max_y).\n        \"\"\"\n        transformer = Transformer.from_crs(\"epsg:4326\", crs, always_xy=True)\n        min_x, min_y = transformer.transform(self.bbox[0], self.bbox[1])\n        max_x, max_y = transformer.transform(self.bbox[2], self.bbox[3])\n        return min_x, min_y, max_x, max_y\n\n    def _calculate_window(self, cog, min_x, min_y, max_x, max_y):\n        \"\"\"\n        Calculate the window for reading the data from the COG.\n\n        Parameters:\n        cog (rasterio.io.DatasetReader): COG dataset reader.\n        min_x (float): Minimum x-coordinate.\n        min_y (float): Minimum y-coordinate.\n        max_x (float): Maximum x-coordinate.\n        max_y (float): Maximum y-coordinate.\n\n        Returns:\n        rasterio.windows.Window: Window for reading the data.\n        \"\"\"\n        return from_bounds(min_x, min_y, max_x, max_y, cog.transform)\n\n    def _is_window_out_of_bounds(self, window):\n        \"\"\"\n        Check if the window is out of bounds.\n\n        Parameters:\n        window (rasterio.windows.Window): Window to check.\n\n        Returns:\n        bool: True if the window is out of bounds, False otherwise.\n        \"\"\"\n        return (\n            window.col_off &lt; 0\n            or window.row_off &lt; 0\n            or window.width &lt;= 0\n            or window.height &lt;= 0\n        )\n\n    def _get_band_urls(self, features):\n        \"\"\"\n        Get the URLs of the bands to be extracted.\n\n        Parameters:\n        features (list): List of features containing the band URLs.\n\n        Returns:\n        list: List of band URLs.\n        \"\"\"\n        band_urls = [\n            [feature[\"assets\"][band][\"href\"] for band in self.bands_list]\n            for feature in features\n        ]\n        return band_urls\n\n    def _fetch_and_save_bands(self, band_urls, feature_id):\n        \"\"\"\n        Fetch and save the bands from the given URLs.\n\n        Parameters:\n        band_urls (list): List of band URLs.\n        feature_id (str): Feature ID for naming the output file.\n\n        Returns:\n        str: Path to the saved GeoTIFF file.\n        \"\"\"\n        try:\n            bands = []\n            bands_meta = []\n            resolutions = []\n\n            for band_url in band_urls:\n                with rasterio.open(band_url) as band_cog:\n                    resolutions.append(band_cog.res)\n\n            lowest_resolution = max(resolutions, key=lambda res: res[0] * res[1])\n\n            for band_url in band_urls:\n                with rasterio.open(band_url) as band_cog:\n                    min_x, min_y, max_x, max_y = self._transform_bbox(band_cog.crs)\n                    band_window = self._calculate_window(\n                        band_cog, min_x, min_y, max_x, max_y\n                    )\n\n                    if self._is_window_out_of_bounds(band_window):\n                        return None\n                    self.crs = band_cog.crs\n                    self.transform = band_cog.transform\n\n                    band_data = band_cog.read(1, window=band_window).astype(float)\n\n                    # Resample if necessary\n                    if band_cog.res != lowest_resolution:\n                        scale_factor_x = band_cog.res[0] / lowest_resolution[0]\n                        scale_factor_y = band_cog.res[1] / lowest_resolution[1]\n                        band_data = reproject(\n                            source=band_data,\n                            destination=np.empty(\n                                (\n                                    int(band_data.shape[0] * scale_factor_y),\n                                    int(band_data.shape[1] * scale_factor_x),\n                                ),\n                                dtype=band_data.dtype,\n                            ),\n                            src_transform=band_cog.transform,\n                            src_crs=band_cog.crs,\n                            dst_transform=band_cog.transform\n                            * band_cog.transform.scale(scale_factor_x, scale_factor_y),\n                            dst_crs=band_cog.crs,\n                            resampling=Resampling.average,\n                        )[0]\n\n                    bands.append(band_data)\n                    bands_meta.append(band_url.split(\"/\")[-1].split(\".\")[0])\n\n            print(\"Stacking Bands...\")\n            stacked_bands = np.stack(bands)\n            output_file = os.path.join(\n                self.output_dir, f\"{feature_id}_bands_export.tif\"\n            )\n            self._save_geotiff(stacked_bands, output_file, bands_meta)\n            return output_file\n        except Exception as ex:\n            print(f\"Error fetching bands: {ex}\")\n            raise ex\n            return None\n\n    def _save_geotiff(self, bands, output_file, bands_meta=None):\n        \"\"\"\n        Save the bands as a GeoTIFF file.\n\n        Parameters:\n        bands (numpy.ndarray): Array of bands to save.\n        output_file (str): Path to the output file.\n        bands_meta (list): List of metadata for the bands.\n        \"\"\"\n\n        band_shape = bands.shape\n        nodata_value = -9999\n        bands = np.where(np.isnan(bands), nodata_value, bands)\n        with rasterio.open(\n            output_file,\n            \"w\",\n            driver=\"GTiff\",\n            height=bands.shape[1],\n            width=bands.shape[2],\n            count=len(bands),\n            dtype=bands.dtype,\n            crs=self.crs,\n            transform=self.transform,\n            nodata=nodata_value,\n        ) as dst:\n            for band in range(1, band_shape[0] + 1):\n                dst.write(bands[band - 1], band)\n                if bands_meta:\n                    dst.set_band_description(band, bands_meta[band - 1])\n\n    def extract(self):\n        \"\"\"\n        Extract the bands from the satellite images and save them as GeoTIFF files.\n        \"\"\"\n        print(\"Extracting bands...\")\n        os.makedirs(self.output_dir, exist_ok=True)\n\n        features = search_stac_api(\n            self.bbox,\n            self.start_date,\n            self.end_date,\n            self.cloud_cover,\n        )\n        print(f\"Total scenes found: {len(features)}\")\n        filtered_features = filter_intersected_features(features, self.bbox)\n        print(f\"Scenes covering input area: {len(filtered_features)}\")\n        overlapping_features_removed = remove_overlapping_sentinel2_tiles(\n            filtered_features\n        )\n        print(f\"Scenes after removing overlaps: {len(overlapping_features_removed)}\")\n        if self.use_smart_filter:\n            overlapping_features_removed = smart_filter_images(\n                overlapping_features_removed, self.start_date, self.end_date\n            )\n            print(\n                f\"Scenes after applying smart filter: {len(overlapping_features_removed)}\"\n            )\n\n        band_urls_list = self._get_band_urls(overlapping_features_removed)\n        result_lists = []\n        if self.workers &gt; 1:\n            print(\"Using Parallel Processing...\")\n            with ThreadPoolExecutor(max_workers=self.workers) as executor:\n                futures = [\n                    executor.submit(\n                        self._fetch_and_save_bands, band_urls, feature[\"id\"]\n                    )\n                    for band_urls, feature in zip(\n                        band_urls_list, overlapping_features_removed\n                    )\n                ]\n                for future in tqdm(\n                    as_completed(futures),\n                    total=len(futures),\n                    desc=\"Extracting Bands\",\n                    file=self.log_file,\n                ):\n                    result = future.result()\n                    result_lists.append(result)\n        else:\n            for band_urls, feature in tqdm(\n                zip(band_urls_list, overlapping_features_removed),\n                total=len(band_urls_list),\n                desc=\"Extracting Bands\",\n                file=self.log_file,\n            ):\n                result = self._fetch_and_save_bands(band_urls, feature[\"id\"])\n                result_lists.append(result)\n        if self.zip_output:\n            zip_files(\n                result_lists,\n                os.path.join(self.output_dir, \"tiff_files.zip\"),\n            )\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor.bbox","title":"<code>bbox = bbox</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.start_date","title":"<code>start_date = start_date</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.end_date","title":"<code>end_date = end_date</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.cloud_cover","title":"<code>cloud_cover = cloud_cover</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.bands_list","title":"<code>bands_list = bands_list</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.output_dir","title":"<code>output_dir = output_dir</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.log_file","title":"<code>log_file = log_file</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.workers","title":"<code>workers = workers</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.zip_output","title":"<code>zip_output = zip_output</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.crs","title":"<code>crs = None</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.transform","title":"<code>transform = None</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.use_smart_filter","title":"<code>use_smart_filter = smart_filter</code>  <code>instance-attribute</code>","text":""},{"location":"src/extract/#virtughan.extract.ExtractProcessor.__init__","title":"<code>__init__(bbox, start_date, end_date, cloud_cover, bands_list, output_dir, log_file=sys.stdout, workers=1, zip_output=False, smart_filter=True)</code>","text":"<p>Initialize the ExtractProcessor.</p> <p>Parameters: bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat]. start_date (str): Start date for the data extraction (YYYY-MM-DD). end_date (str): End date for the data extraction (YYYY-MM-DD). cloud_cover (int): Maximum allowed cloud cover percentage. bands_list (list): List of bands to extract. output_dir (str): Directory to save the extracted bands. log_file (file): File to log the extraction process. workers (int): Number of parallel workers. zip_output (bool): Whether to zip the output files. smart_filter (bool): Whether to apply smart filtering to the images.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def __init__(\n    self,\n    bbox,\n    start_date,\n    end_date,\n    cloud_cover,\n    bands_list,\n    output_dir,\n    log_file=sys.stdout,\n    workers=1,\n    zip_output=False,\n    smart_filter=True,\n):\n    \"\"\"\n    Initialize the ExtractProcessor.\n\n    Parameters:\n    bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat].\n    start_date (str): Start date for the data extraction (YYYY-MM-DD).\n    end_date (str): End date for the data extraction (YYYY-MM-DD).\n    cloud_cover (int): Maximum allowed cloud cover percentage.\n    bands_list (list): List of bands to extract.\n    output_dir (str): Directory to save the extracted bands.\n    log_file (file): File to log the extraction process.\n    workers (int): Number of parallel workers.\n    zip_output (bool): Whether to zip the output files.\n    smart_filter (bool): Whether to apply smart filtering to the images.\n    \"\"\"\n    self.bbox = bbox\n    self.start_date = start_date\n    self.end_date = end_date\n    self.cloud_cover = cloud_cover\n    self.bands_list = bands_list\n    self.output_dir = output_dir\n    self.log_file = log_file\n    self.workers = workers\n    self.zip_output = zip_output\n    self.crs = None\n    self.transform = None\n    self.use_smart_filter = smart_filter\n\n    self._validate_bands_list()\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor._validate_bands_list","title":"<code>_validate_bands_list()</code>","text":"<p>Validate the list of bands to ensure they are valid.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def _validate_bands_list(self):\n    \"\"\"\n    Validate the list of bands to ensure they are valid.\n    \"\"\"\n    invalid_bands = [band for band in self.bands_list if band not in VALID_BANDS]\n    if invalid_bands:\n        raise ValueError(\n            f\"Invalid band names: {', '.join(invalid_bands)}. \"\n            f\"Band names should be one of: {', '.join(VALID_BANDS.keys())}\"\n        )\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor._transform_bbox","title":"<code>_transform_bbox(crs)</code>","text":"<p>Transform the bounding box coordinates to the specified CRS.</p> <p>Parameters: crs (str): Coordinate reference system to transform to.</p> <p>Returns: tuple: Transformed bounding box coordinates (min_x, min_y, max_x, max_y).</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def _transform_bbox(self, crs):\n    \"\"\"\n    Transform the bounding box coordinates to the specified CRS.\n\n    Parameters:\n    crs (str): Coordinate reference system to transform to.\n\n    Returns:\n    tuple: Transformed bounding box coordinates (min_x, min_y, max_x, max_y).\n    \"\"\"\n    transformer = Transformer.from_crs(\"epsg:4326\", crs, always_xy=True)\n    min_x, min_y = transformer.transform(self.bbox[0], self.bbox[1])\n    max_x, max_y = transformer.transform(self.bbox[2], self.bbox[3])\n    return min_x, min_y, max_x, max_y\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor._calculate_window","title":"<code>_calculate_window(cog, min_x, min_y, max_x, max_y)</code>","text":"<p>Calculate the window for reading the data from the COG.</p> <p>Parameters: cog (rasterio.io.DatasetReader): COG dataset reader. min_x (float): Minimum x-coordinate. min_y (float): Minimum y-coordinate. max_x (float): Maximum x-coordinate. max_y (float): Maximum y-coordinate.</p> <p>Returns: rasterio.windows.Window: Window for reading the data.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def _calculate_window(self, cog, min_x, min_y, max_x, max_y):\n    \"\"\"\n    Calculate the window for reading the data from the COG.\n\n    Parameters:\n    cog (rasterio.io.DatasetReader): COG dataset reader.\n    min_x (float): Minimum x-coordinate.\n    min_y (float): Minimum y-coordinate.\n    max_x (float): Maximum x-coordinate.\n    max_y (float): Maximum y-coordinate.\n\n    Returns:\n    rasterio.windows.Window: Window for reading the data.\n    \"\"\"\n    return from_bounds(min_x, min_y, max_x, max_y, cog.transform)\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor._is_window_out_of_bounds","title":"<code>_is_window_out_of_bounds(window)</code>","text":"<p>Check if the window is out of bounds.</p> <p>Parameters: window (rasterio.windows.Window): Window to check.</p> <p>Returns: bool: True if the window is out of bounds, False otherwise.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def _is_window_out_of_bounds(self, window):\n    \"\"\"\n    Check if the window is out of bounds.\n\n    Parameters:\n    window (rasterio.windows.Window): Window to check.\n\n    Returns:\n    bool: True if the window is out of bounds, False otherwise.\n    \"\"\"\n    return (\n        window.col_off &lt; 0\n        or window.row_off &lt; 0\n        or window.width &lt;= 0\n        or window.height &lt;= 0\n    )\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor._get_band_urls","title":"<code>_get_band_urls(features)</code>","text":"<p>Get the URLs of the bands to be extracted.</p> <p>Parameters: features (list): List of features containing the band URLs.</p> <p>Returns: list: List of band URLs.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def _get_band_urls(self, features):\n    \"\"\"\n    Get the URLs of the bands to be extracted.\n\n    Parameters:\n    features (list): List of features containing the band URLs.\n\n    Returns:\n    list: List of band URLs.\n    \"\"\"\n    band_urls = [\n        [feature[\"assets\"][band][\"href\"] for band in self.bands_list]\n        for feature in features\n    ]\n    return band_urls\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor._fetch_and_save_bands","title":"<code>_fetch_and_save_bands(band_urls, feature_id)</code>","text":"<p>Fetch and save the bands from the given URLs.</p> <p>Parameters: band_urls (list): List of band URLs. feature_id (str): Feature ID for naming the output file.</p> <p>Returns: str: Path to the saved GeoTIFF file.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def _fetch_and_save_bands(self, band_urls, feature_id):\n    \"\"\"\n    Fetch and save the bands from the given URLs.\n\n    Parameters:\n    band_urls (list): List of band URLs.\n    feature_id (str): Feature ID for naming the output file.\n\n    Returns:\n    str: Path to the saved GeoTIFF file.\n    \"\"\"\n    try:\n        bands = []\n        bands_meta = []\n        resolutions = []\n\n        for band_url in band_urls:\n            with rasterio.open(band_url) as band_cog:\n                resolutions.append(band_cog.res)\n\n        lowest_resolution = max(resolutions, key=lambda res: res[0] * res[1])\n\n        for band_url in band_urls:\n            with rasterio.open(band_url) as band_cog:\n                min_x, min_y, max_x, max_y = self._transform_bbox(band_cog.crs)\n                band_window = self._calculate_window(\n                    band_cog, min_x, min_y, max_x, max_y\n                )\n\n                if self._is_window_out_of_bounds(band_window):\n                    return None\n                self.crs = band_cog.crs\n                self.transform = band_cog.transform\n\n                band_data = band_cog.read(1, window=band_window).astype(float)\n\n                # Resample if necessary\n                if band_cog.res != lowest_resolution:\n                    scale_factor_x = band_cog.res[0] / lowest_resolution[0]\n                    scale_factor_y = band_cog.res[1] / lowest_resolution[1]\n                    band_data = reproject(\n                        source=band_data,\n                        destination=np.empty(\n                            (\n                                int(band_data.shape[0] * scale_factor_y),\n                                int(band_data.shape[1] * scale_factor_x),\n                            ),\n                            dtype=band_data.dtype,\n                        ),\n                        src_transform=band_cog.transform,\n                        src_crs=band_cog.crs,\n                        dst_transform=band_cog.transform\n                        * band_cog.transform.scale(scale_factor_x, scale_factor_y),\n                        dst_crs=band_cog.crs,\n                        resampling=Resampling.average,\n                    )[0]\n\n                bands.append(band_data)\n                bands_meta.append(band_url.split(\"/\")[-1].split(\".\")[0])\n\n        print(\"Stacking Bands...\")\n        stacked_bands = np.stack(bands)\n        output_file = os.path.join(\n            self.output_dir, f\"{feature_id}_bands_export.tif\"\n        )\n        self._save_geotiff(stacked_bands, output_file, bands_meta)\n        return output_file\n    except Exception as ex:\n        print(f\"Error fetching bands: {ex}\")\n        raise ex\n        return None\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor._save_geotiff","title":"<code>_save_geotiff(bands, output_file, bands_meta=None)</code>","text":"<p>Save the bands as a GeoTIFF file.</p> <p>Parameters: bands (numpy.ndarray): Array of bands to save. output_file (str): Path to the output file. bands_meta (list): List of metadata for the bands.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def _save_geotiff(self, bands, output_file, bands_meta=None):\n    \"\"\"\n    Save the bands as a GeoTIFF file.\n\n    Parameters:\n    bands (numpy.ndarray): Array of bands to save.\n    output_file (str): Path to the output file.\n    bands_meta (list): List of metadata for the bands.\n    \"\"\"\n\n    band_shape = bands.shape\n    nodata_value = -9999\n    bands = np.where(np.isnan(bands), nodata_value, bands)\n    with rasterio.open(\n        output_file,\n        \"w\",\n        driver=\"GTiff\",\n        height=bands.shape[1],\n        width=bands.shape[2],\n        count=len(bands),\n        dtype=bands.dtype,\n        crs=self.crs,\n        transform=self.transform,\n        nodata=nodata_value,\n    ) as dst:\n        for band in range(1, band_shape[0] + 1):\n            dst.write(bands[band - 1], band)\n            if bands_meta:\n                dst.set_band_description(band, bands_meta[band - 1])\n</code></pre>"},{"location":"src/extract/#virtughan.extract.ExtractProcessor.extract","title":"<code>extract()</code>","text":"<p>Extract the bands from the satellite images and save them as GeoTIFF files.</p> Source code in <code>src/virtughan/extract.py</code> <pre><code>def extract(self):\n    \"\"\"\n    Extract the bands from the satellite images and save them as GeoTIFF files.\n    \"\"\"\n    print(\"Extracting bands...\")\n    os.makedirs(self.output_dir, exist_ok=True)\n\n    features = search_stac_api(\n        self.bbox,\n        self.start_date,\n        self.end_date,\n        self.cloud_cover,\n    )\n    print(f\"Total scenes found: {len(features)}\")\n    filtered_features = filter_intersected_features(features, self.bbox)\n    print(f\"Scenes covering input area: {len(filtered_features)}\")\n    overlapping_features_removed = remove_overlapping_sentinel2_tiles(\n        filtered_features\n    )\n    print(f\"Scenes after removing overlaps: {len(overlapping_features_removed)}\")\n    if self.use_smart_filter:\n        overlapping_features_removed = smart_filter_images(\n            overlapping_features_removed, self.start_date, self.end_date\n        )\n        print(\n            f\"Scenes after applying smart filter: {len(overlapping_features_removed)}\"\n        )\n\n    band_urls_list = self._get_band_urls(overlapping_features_removed)\n    result_lists = []\n    if self.workers &gt; 1:\n        print(\"Using Parallel Processing...\")\n        with ThreadPoolExecutor(max_workers=self.workers) as executor:\n            futures = [\n                executor.submit(\n                    self._fetch_and_save_bands, band_urls, feature[\"id\"]\n                )\n                for band_urls, feature in zip(\n                    band_urls_list, overlapping_features_removed\n                )\n            ]\n            for future in tqdm(\n                as_completed(futures),\n                total=len(futures),\n                desc=\"Extracting Bands\",\n                file=self.log_file,\n            ):\n                result = future.result()\n                result_lists.append(result)\n    else:\n        for band_urls, feature in tqdm(\n            zip(band_urls_list, overlapping_features_removed),\n            total=len(band_urls_list),\n            desc=\"Extracting Bands\",\n            file=self.log_file,\n        ):\n            result = self._fetch_and_save_bands(band_urls, feature[\"id\"])\n            result_lists.append(result)\n    if self.zip_output:\n        zip_files(\n            result_lists,\n            os.path.join(self.output_dir, \"tiff_files.zip\"),\n        )\n</code></pre>"},{"location":"src/tile/","title":"Tile Module","text":""},{"location":"src/tile/#virtughan.tile","title":"<code>virtughan.tile</code>","text":""},{"location":"src/tile/#virtughan.tile.TileProcessor","title":"<code>TileProcessor</code>","text":"<p>Processor for generating and caching tiles from satellite images.</p> Source code in <code>src/virtughan/tile.py</code> <pre><code>class TileProcessor:\n    \"\"\"\n    Processor for generating and caching tiles from satellite images.\n    \"\"\"\n\n    def __init__(self, cache_time=60):\n        \"\"\"\n        Initialize the TileProcessor.\n\n        Parameters:\n        cache_time (int): Cache time in seconds.\n        \"\"\"\n        self.cache_time = cache_time\n\n    @staticmethod\n    def apply_colormap(result, colormap_str):\n        \"\"\"\n        Apply a colormap to the result.\n\n        Parameters:\n        result (numpy.ndarray): Array of results to apply the colormap to.\n        colormap_str (str): Name of the colormap to apply.\n\n        Returns:\n        PIL.Image.Image: Image with the applied colormap.\n        \"\"\"\n        result_normalized = (result - result.min()) / (result.max() - result.min())\n        colormap = plt.get_cmap(colormap_str)\n        result_colored = colormap(result_normalized)\n        result_image = (result_colored[:, :, :3] * 255).astype(np.uint8)\n        return Image.fromarray(result_image)\n\n    @staticmethod\n    async def fetch_tile(url, x, y, z):\n        \"\"\"\n        Fetch a tile from the given URL.\n\n        Parameters:\n        url (str): URL of the tile.\n        x (int): X coordinate of the tile.\n        y (int): Y coordinate of the tile.\n        z (int): Zoom level of the tile.\n\n        Returns:\n        numpy.ndarray: Array of the tile data.\n        \"\"\"\n\n        def read_tile():\n            with COGReader(url) as cog:\n                tile, _ = cog.tile(x, y, z)\n                return tile\n\n        return await asyncio.to_thread(read_tile)\n\n    @cached(ttl=60 * 1)\n    async def cached_generate_tile(\n        self,\n        x: int,\n        y: int,\n        z: int,\n        start_date: str,\n        end_date: str,\n        cloud_cover: int,\n        band1: str,\n        band2: str,\n        formula: str,\n        colormap_str: str = \"RdYlGn\",\n        latest: bool = True,\n        operation: str = \"median\",\n    ) -&gt; bytes:\n        \"\"\"\n        Generate and cache a tile.\n\n        Parameters:\n        x (int): X coordinate of the tile.\n        y (int): Y coordinate of the tile.\n        z (int): Zoom level of the tile.\n        start_date (str): Start date for the data extraction (YYYY-MM-DD).\n        end_date (str): End date for the data extraction (YYYY-MM-DD).\n        cloud_cover (int): Maximum allowed cloud cover percentage.\n        band1 (str): First band for the formula.\n        band2 (str): Second band for the formula.\n        formula (str): Formula to apply to the bands.\n        colormap_str (str): Name of the colormap to apply.\n        latest (bool): Whether to use the latest image.\n        operation (str): Operation to apply to the time series.\n\n        Returns:\n        bytes: Image bytes of the generated tile.\n        \"\"\"\n        tile = mercantile.Tile(x, y, z)\n        bbox = mercantile.bounds(tile)\n        bbox_geojson = mapping(box(bbox.west, bbox.south, bbox.east, bbox.north))\n        results = await search_stac_api_async(\n            bbox_geojson, start_date, end_date, cloud_cover\n        )\n\n        if not results:\n            raise HTTPException(\n                status_code=404, detail=\"No images found for the given parameters\"\n            )\n\n        results = filter_intersected_features(\n            results, [bbox.west, bbox.south, bbox.east, bbox.north]\n        )\n        if latest:\n            if len(results) &gt; 0:\n                results = filter_latest_image_per_grid(results)\n                feature = results[0]\n                band1_url = feature[\"assets\"][band1][\"href\"]\n                band2_url = feature[\"assets\"][band2][\"href\"] if band2 else None\n\n                try:\n                    tasks = [self.fetch_tile(band1_url, x, y, z)]\n                    if band2_url:\n                        tasks.append(self.fetch_tile(band2_url, x, y, z))\n\n                    tiles = await asyncio.gather(*tasks)\n                    band1 = tiles[0]\n                    band2 = tiles[1] if band2_url else None\n                except Exception as e:\n                    raise HTTPException(status_code=500, detail=str(e))\n\n                if band2 is not None:\n                    band1 = band1[0].astype(float)\n                    band2 = band2[0].astype(float)\n                    result = eval(formula)\n                    image = self.apply_colormap(result, colormap_str)\n                else:\n                    inner_bands = band1.shape[0]\n                    if inner_bands == 1:\n                        band1 = band1[0].astype(float)\n                        result = eval(formula)\n                        image = self.apply_colormap(result, colormap_str)\n                    else:\n                        band1 = band1.transpose(1, 2, 0)\n                        image = Image.fromarray(band1)\n            else:\n\n                raise HTTPException(\n                    status_code=404, detail=\"No images found for the given parameters\"\n                )\n        else:\n\n            results = remove_overlapping_sentinel2_tiles(results)\n            results = smart_filter_images(results, start_date, end_date)\n            band1_tiles = []\n            band2_tiles = []\n\n            tasks = []\n            for feature in results:\n                band1_url = feature[\"assets\"][band1][\"href\"]\n                band2_url = feature[\"assets\"][band2][\"href\"] if band2 else None\n                tasks.append(self.fetch_tile(band1_url, x, y, z))\n                if band2_url:\n                    tasks.append(self.fetch_tile(band2_url, x, y, z))\n\n            try:\n                tiles = await asyncio.gather(*tasks)\n                for i in range(0, len(tiles), 2 if band2 else 1):\n                    band1_tiles.append(tiles[i])\n                    if band2:\n                        band2_tiles.append(tiles[i + 1])\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=str(e))\n\n            band1 = aggregate_time_series(\n                [tile[0].astype(float) for tile in band1_tiles], operation\n            )\n            if band2_tiles:\n                band2 = aggregate_time_series(\n                    [tile[0].astype(float) for tile in band2_tiles], operation\n                )\n                result = eval(formula)\n            else:\n                result = eval(formula)\n\n            image = self.apply_colormap(result, colormap_str)\n\n        buffered = BytesIO()\n        image.save(buffered, format=\"PNG\")\n        image_bytes = buffered.getvalue()\n\n        return image_bytes, feature\n</code></pre>"},{"location":"src/tile/#virtughan.tile.TileProcessor.cache_time","title":"<code>cache_time = cache_time</code>  <code>instance-attribute</code>","text":""},{"location":"src/tile/#virtughan.tile.TileProcessor.__init__","title":"<code>__init__(cache_time=60)</code>","text":"<p>Initialize the TileProcessor.</p> <p>Parameters: cache_time (int): Cache time in seconds.</p> Source code in <code>src/virtughan/tile.py</code> <pre><code>def __init__(self, cache_time=60):\n    \"\"\"\n    Initialize the TileProcessor.\n\n    Parameters:\n    cache_time (int): Cache time in seconds.\n    \"\"\"\n    self.cache_time = cache_time\n</code></pre>"},{"location":"src/tile/#virtughan.tile.TileProcessor.apply_colormap","title":"<code>apply_colormap(result, colormap_str)</code>  <code>staticmethod</code>","text":"<p>Apply a colormap to the result.</p> <p>Parameters: result (numpy.ndarray): Array of results to apply the colormap to. colormap_str (str): Name of the colormap to apply.</p> <p>Returns: PIL.Image.Image: Image with the applied colormap.</p> Source code in <code>src/virtughan/tile.py</code> <pre><code>@staticmethod\ndef apply_colormap(result, colormap_str):\n    \"\"\"\n    Apply a colormap to the result.\n\n    Parameters:\n    result (numpy.ndarray): Array of results to apply the colormap to.\n    colormap_str (str): Name of the colormap to apply.\n\n    Returns:\n    PIL.Image.Image: Image with the applied colormap.\n    \"\"\"\n    result_normalized = (result - result.min()) / (result.max() - result.min())\n    colormap = plt.get_cmap(colormap_str)\n    result_colored = colormap(result_normalized)\n    result_image = (result_colored[:, :, :3] * 255).astype(np.uint8)\n    return Image.fromarray(result_image)\n</code></pre>"},{"location":"src/tile/#virtughan.tile.TileProcessor.fetch_tile","title":"<code>fetch_tile(url, x, y, z)</code>  <code>async</code> <code>staticmethod</code>","text":"<p>Fetch a tile from the given URL.</p> <p>Parameters: url (str): URL of the tile. x (int): X coordinate of the tile. y (int): Y coordinate of the tile. z (int): Zoom level of the tile.</p> <p>Returns: numpy.ndarray: Array of the tile data.</p> Source code in <code>src/virtughan/tile.py</code> <pre><code>@staticmethod\nasync def fetch_tile(url, x, y, z):\n    \"\"\"\n    Fetch a tile from the given URL.\n\n    Parameters:\n    url (str): URL of the tile.\n    x (int): X coordinate of the tile.\n    y (int): Y coordinate of the tile.\n    z (int): Zoom level of the tile.\n\n    Returns:\n    numpy.ndarray: Array of the tile data.\n    \"\"\"\n\n    def read_tile():\n        with COGReader(url) as cog:\n            tile, _ = cog.tile(x, y, z)\n            return tile\n\n    return await asyncio.to_thread(read_tile)\n</code></pre>"},{"location":"src/tile/#virtughan.tile.TileProcessor.cached_generate_tile","title":"<code>cached_generate_tile(x: int, y: int, z: int, start_date: str, end_date: str, cloud_cover: int, band1: str, band2: str, formula: str, colormap_str: str = 'RdYlGn', latest: bool = True, operation: str = 'median') -&gt; bytes</code>  <code>async</code>","text":"<p>Generate and cache a tile.</p> <p>Parameters: x (int): X coordinate of the tile. y (int): Y coordinate of the tile. z (int): Zoom level of the tile. start_date (str): Start date for the data extraction (YYYY-MM-DD). end_date (str): End date for the data extraction (YYYY-MM-DD). cloud_cover (int): Maximum allowed cloud cover percentage. band1 (str): First band for the formula. band2 (str): Second band for the formula. formula (str): Formula to apply to the bands. colormap_str (str): Name of the colormap to apply. latest (bool): Whether to use the latest image. operation (str): Operation to apply to the time series.</p> <p>Returns: bytes: Image bytes of the generated tile.</p> Source code in <code>src/virtughan/tile.py</code> <pre><code>@cached(ttl=60 * 1)\nasync def cached_generate_tile(\n    self,\n    x: int,\n    y: int,\n    z: int,\n    start_date: str,\n    end_date: str,\n    cloud_cover: int,\n    band1: str,\n    band2: str,\n    formula: str,\n    colormap_str: str = \"RdYlGn\",\n    latest: bool = True,\n    operation: str = \"median\",\n) -&gt; bytes:\n    \"\"\"\n    Generate and cache a tile.\n\n    Parameters:\n    x (int): X coordinate of the tile.\n    y (int): Y coordinate of the tile.\n    z (int): Zoom level of the tile.\n    start_date (str): Start date for the data extraction (YYYY-MM-DD).\n    end_date (str): End date for the data extraction (YYYY-MM-DD).\n    cloud_cover (int): Maximum allowed cloud cover percentage.\n    band1 (str): First band for the formula.\n    band2 (str): Second band for the formula.\n    formula (str): Formula to apply to the bands.\n    colormap_str (str): Name of the colormap to apply.\n    latest (bool): Whether to use the latest image.\n    operation (str): Operation to apply to the time series.\n\n    Returns:\n    bytes: Image bytes of the generated tile.\n    \"\"\"\n    tile = mercantile.Tile(x, y, z)\n    bbox = mercantile.bounds(tile)\n    bbox_geojson = mapping(box(bbox.west, bbox.south, bbox.east, bbox.north))\n    results = await search_stac_api_async(\n        bbox_geojson, start_date, end_date, cloud_cover\n    )\n\n    if not results:\n        raise HTTPException(\n            status_code=404, detail=\"No images found for the given parameters\"\n        )\n\n    results = filter_intersected_features(\n        results, [bbox.west, bbox.south, bbox.east, bbox.north]\n    )\n    if latest:\n        if len(results) &gt; 0:\n            results = filter_latest_image_per_grid(results)\n            feature = results[0]\n            band1_url = feature[\"assets\"][band1][\"href\"]\n            band2_url = feature[\"assets\"][band2][\"href\"] if band2 else None\n\n            try:\n                tasks = [self.fetch_tile(band1_url, x, y, z)]\n                if band2_url:\n                    tasks.append(self.fetch_tile(band2_url, x, y, z))\n\n                tiles = await asyncio.gather(*tasks)\n                band1 = tiles[0]\n                band2 = tiles[1] if band2_url else None\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=str(e))\n\n            if band2 is not None:\n                band1 = band1[0].astype(float)\n                band2 = band2[0].astype(float)\n                result = eval(formula)\n                image = self.apply_colormap(result, colormap_str)\n            else:\n                inner_bands = band1.shape[0]\n                if inner_bands == 1:\n                    band1 = band1[0].astype(float)\n                    result = eval(formula)\n                    image = self.apply_colormap(result, colormap_str)\n                else:\n                    band1 = band1.transpose(1, 2, 0)\n                    image = Image.fromarray(band1)\n        else:\n\n            raise HTTPException(\n                status_code=404, detail=\"No images found for the given parameters\"\n            )\n    else:\n\n        results = remove_overlapping_sentinel2_tiles(results)\n        results = smart_filter_images(results, start_date, end_date)\n        band1_tiles = []\n        band2_tiles = []\n\n        tasks = []\n        for feature in results:\n            band1_url = feature[\"assets\"][band1][\"href\"]\n            band2_url = feature[\"assets\"][band2][\"href\"] if band2 else None\n            tasks.append(self.fetch_tile(band1_url, x, y, z))\n            if band2_url:\n                tasks.append(self.fetch_tile(band2_url, x, y, z))\n\n        try:\n            tiles = await asyncio.gather(*tasks)\n            for i in range(0, len(tiles), 2 if band2 else 1):\n                band1_tiles.append(tiles[i])\n                if band2:\n                    band2_tiles.append(tiles[i + 1])\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n\n        band1 = aggregate_time_series(\n            [tile[0].astype(float) for tile in band1_tiles], operation\n        )\n        if band2_tiles:\n            band2 = aggregate_time_series(\n                [tile[0].astype(float) for tile in band2_tiles], operation\n            )\n            result = eval(formula)\n        else:\n            result = eval(formula)\n\n        image = self.apply_colormap(result, colormap_str)\n\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    image_bytes = buffered.getvalue()\n\n    return image_bytes, feature\n</code></pre>"},{"location":"src/utis/","title":"Utils Module","text":""},{"location":"src/utis/#virtughan.utils","title":"<code>virtughan.utils</code>","text":""},{"location":"src/utis/#virtughan.utils.STAC_API_URL","title":"<code>STAC_API_URL = 'https://earth-search.aws.element84.com/v1/search'</code>  <code>module-attribute</code>","text":""},{"location":"src/utis/#virtughan.utils.search_stac_api","title":"<code>search_stac_api(bbox, start_date, end_date, cloud_cover)</code>","text":"<p>Search the STAC API for satellite images.</p> <p>Parameters: bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat]. start_date (str): Start date for the search (YYYY-MM-DD). end_date (str): End date for the search (YYYY-MM-DD). cloud_cover (int): Maximum allowed cloud cover percentage.</p> <p>Returns: list: List of features found in the search.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>def search_stac_api(bbox, start_date, end_date, cloud_cover):\n    \"\"\"\n    Search the STAC API for satellite images.\n\n    Parameters:\n    bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat].\n    start_date (str): Start date for the search (YYYY-MM-DD).\n    end_date (str): End date for the search (YYYY-MM-DD).\n    cloud_cover (int): Maximum allowed cloud cover percentage.\n\n    Returns:\n    list: List of features found in the search.\n    \"\"\"\n    search_params = {\n        \"collections\": [\"sentinel-2-l2a\"],\n        \"datetime\": f\"{start_date}T00:00:00Z/{end_date}T23:59:59Z\",\n        \"query\": {\"eo:cloud_cover\": {\"lt\": cloud_cover}},\n        \"bbox\": bbox,\n        \"limit\": 100,\n        \"sortby\": [{\"field\": \"properties.datetime\", \"direction\": \"desc\"}],\n    }\n\n    all_features = []\n    next_link = None\n\n    while True:\n        response = requests.post(\n            STAC_API_URL,\n            json=search_params if not next_link else next_link[\"body\"],\n        )\n        response.raise_for_status()\n        response_json = response.json()\n\n        all_features.extend(response_json[\"features\"])\n\n        next_link = next(\n            (link for link in response_json[\"links\"] if link[\"rel\"] == \"next\"), None\n        )\n        if not next_link:\n            break\n    return all_features\n</code></pre>"},{"location":"src/utis/#virtughan.utils.search_stac_api_async","title":"<code>search_stac_api_async(bbox_geojson, start_date, end_date, cloud_cover)</code>  <code>async</code>","text":"<p>Asynchronously search the STAC API for satellite images.</p> <p>Parameters: bbox_geojson (dict): Bounding box in GeoJSON format. start_date (str): Start date for the search (YYYY-MM-DD). end_date (str): End date for the search (YYYY-MM-DD). cloud_cover (int): Maximum allowed cloud cover percentage.</p> <p>Returns: list: List of features found in the search.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>async def search_stac_api_async(bbox_geojson, start_date, end_date, cloud_cover):\n    \"\"\"\n    Asynchronously search the STAC API for satellite images.\n\n    Parameters:\n    bbox_geojson (dict): Bounding box in GeoJSON format.\n    start_date (str): Start date for the search (YYYY-MM-DD).\n    end_date (str): End date for the search (YYYY-MM-DD).\n    cloud_cover (int): Maximum allowed cloud cover percentage.\n\n    Returns:\n    list: List of features found in the search.\n    \"\"\"\n    search_params = {\n        \"collections\": [\"sentinel-2-l2a\"],\n        \"datetime\": f\"{start_date}T00:00:00Z/{end_date}T23:59:59Z\",\n        \"query\": {\"eo:cloud_cover\": {\"lt\": cloud_cover}},\n        \"intersects\": bbox_geojson,\n        \"limit\": 100,\n        \"sortby\": [{\"field\": \"properties.datetime\", \"direction\": \"desc\"}],\n    }\n\n    all_features = []\n    next_link = None\n\n    async with httpx.AsyncClient() as client:\n        while True:\n            response = await client.post(\n                STAC_API_URL,\n                json=search_params if not next_link else next_link[\"body\"],\n            )\n            response.raise_for_status()\n            response_json = response.json()\n\n            all_features.extend(response_json[\"features\"])\n\n            next_link = next(\n                (link for link in response_json[\"links\"] if link[\"rel\"] == \"next\"), None\n            )\n            if not next_link:\n                break\n\n    return all_features\n</code></pre>"},{"location":"src/utis/#virtughan.utils.zip_files","title":"<code>zip_files(file_list, zip_path)</code>","text":"<p>Zip a list of files.</p> <p>Parameters: file_list (list): List of file paths to zip. zip_path (str): Path to the output zip file.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>def zip_files(file_list, zip_path):\n    \"\"\"\n    Zip a list of files.\n\n    Parameters:\n    file_list (list): List of file paths to zip.\n    zip_path (str): Path to the output zip file.\n    \"\"\"\n    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n        for file in file_list:\n            zipf.write(file, os.path.basename(file))\n    print(f\"Saved intermediate images ZIP to {zip_path}\")\n    for file in file_list:\n        os.remove(file)\n</code></pre>"},{"location":"src/utis/#virtughan.utils.filter_latest_image_per_grid","title":"<code>filter_latest_image_per_grid(features)</code>","text":"<p>Filter the latest image per grid.</p> <p>Parameters: features (list): List of features to filter.</p> <p>Returns: list: List of filtered features.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>def filter_latest_image_per_grid(features):\n    \"\"\"\n    Filter the latest image per grid.\n\n    Parameters:\n    features (list): List of features to filter.\n\n    Returns:\n    list: List of filtered features.\n    \"\"\"\n    grid_latest = {}\n    for feature in features:\n        grid = feature[\"id\"].split(\"_\")[1]\n        date = feature[\"properties\"][\"datetime\"]\n        if (\n            grid not in grid_latest\n            or date &gt; grid_latest[grid][\"properties\"][\"datetime\"]\n        ):\n            grid_latest[grid] = feature\n    return list(grid_latest.values())\n</code></pre>"},{"location":"src/utis/#virtughan.utils.filter_intersected_features","title":"<code>filter_intersected_features(features, bbox)</code>","text":"<p>Filter features that intersect with the bounding box.</p> <p>Parameters: features (list): List of features to filter. bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat].</p> <p>Returns: list: List of filtered features.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>def filter_intersected_features(features, bbox):\n    \"\"\"\n    Filter features that intersect with the bounding box.\n\n    Parameters:\n    features (list): List of features to filter.\n    bbox (list): Bounding box coordinates [min_lon, min_lat, max_lon, max_lat].\n\n    Returns:\n    list: List of filtered features.\n    \"\"\"\n    bbox_polygon = box(bbox[0], bbox[1], bbox[2], bbox[3])\n    return [\n        feature\n        for feature in features\n        if shape(feature[\"geometry\"]).contains(bbox_polygon)\n    ]\n</code></pre>"},{"location":"src/utis/#virtughan.utils.remove_overlapping_sentinel2_tiles","title":"<code>remove_overlapping_sentinel2_tiles(features)</code>","text":"<p>Remove overlapping Sentinel-2 tiles.</p> <p>Parameters: features (list): List of features to process.</p> <p>Returns: list: List of non-overlapping features.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>def remove_overlapping_sentinel2_tiles(features):\n    \"\"\"\n    Remove overlapping Sentinel-2 tiles.\n\n    Parameters:\n    features (list): List of features to process.\n\n    Returns:\n    list: List of non-overlapping features.\n    \"\"\"\n    if not features:\n        return []\n\n    zone_counts = {}\n    for feature in features:\n        zone = feature[\"id\"].split(\"_\")[1][:2]\n        zone_counts[zone] = zone_counts.get(zone, 0) + 1\n\n    if not zone_counts:\n        return []\n\n    max_zone = max(zone_counts, key=zone_counts.get)\n\n    filtered_features = {}\n    for feature in features:\n        parts = feature[\"id\"].split(\"_\")\n        date = parts[2]\n        zone = parts[1][:2]\n\n        if zone == max_zone and date not in filtered_features:\n            filtered_features[date] = feature\n\n    return list(filtered_features.values())\n</code></pre>"},{"location":"src/utis/#virtughan.utils.aggregate_time_series","title":"<code>aggregate_time_series(data, operation)</code>","text":"<p>Aggregate a time series of data.</p> <p>Parameters: data (list): List of data arrays to aggregate. operation (str): Operation to apply to the data (mean, median, max, min, std, sum, var).</p> <p>Returns: numpy.ndarray: Aggregated result.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>def aggregate_time_series(data, operation):\n    \"\"\"\n    Aggregate a time series of data.\n\n    Parameters:\n    data (list): List of data arrays to aggregate.\n    operation (str): Operation to apply to the data (mean, median, max, min, std, sum, var).\n\n    Returns:\n    numpy.ndarray: Aggregated result.\n    \"\"\"\n    result_stack = np.ma.stack(data)\n\n    operations = {\n        \"mean\": np.ma.mean,\n        \"median\": np.ma.median,\n        \"max\": np.ma.max,\n        \"min\": np.ma.min,\n        \"std\": np.ma.std,\n        \"sum\": np.ma.sum,\n        \"var\": np.ma.var,\n    }\n\n    return operations[operation](result_stack, axis=0)\n</code></pre>"},{"location":"src/utis/#virtughan.utils.smart_filter_images","title":"<code>smart_filter_images(features, start_date: str, end_date: str)</code>","text":"<p>Apply smart filtering to the image collection , reduces the number of images go through in large timestamps.</p> <p>Parameters: features (list): List of features to filter. start_date (str): Start date for the filtering (YYYY-MM-DD). end_date (str): End date for the filtering (YYYY-MM-DD).</p> <p>Returns: list: List of filtered features.</p> Source code in <code>src/virtughan/utils.py</code> <pre><code>def smart_filter_images(features, start_date: str, end_date: str):\n    \"\"\"\n    Apply smart filtering to the image collection , reduces the number of images go through in large timestamps.\n\n    Parameters:\n    features (list): List of features to filter.\n    start_date (str): Start date for the filtering (YYYY-MM-DD).\n    end_date (str): End date for the filtering (YYYY-MM-DD).\n\n    Returns:\n    list: List of filtered features.\n    \"\"\"\n    start = datetime.fromisoformat(start_date)\n    end = datetime.fromisoformat(end_date)\n    total_days = (end - start).days\n\n    if total_days &lt;= 30 * 3:\n        # For a time range of up to 3 months, select 1 image per 4 days\n        frequency = timedelta(days=4)\n\n    elif total_days &lt;= 365:\n        frequency = timedelta(days=15)\n\n    elif total_days &lt;= 2 * 365:\n        frequency = timedelta(days=30)\n\n    elif total_days &lt;= 3 * 365:\n        frequency = timedelta(days=45)\n    else:\n        # rest, select 1 image per 2 months\n        frequency = timedelta(days=60)\n\n    filtered_features = []\n    last_selected_date = None\n    best_feature = None\n    print(\n        f\"\"\"Filter from : {features[-1][\"properties\"][\"datetime\"].split(\"T\")[0]} to : {features[0][\"properties\"][\"datetime\"].split(\"T\")[0]}\"\"\"\n    )\n    print(f\"Selecting 1 image per {frequency.days} days\")\n\n    for feature in sorted(features, key=lambda x: x[\"properties\"][\"datetime\"]):\n        date = datetime.fromisoformat(feature[\"properties\"][\"datetime\"].split(\"T\")[0])\n        if last_selected_date is None or date &gt;= last_selected_date + frequency:\n            if best_feature:\n                filtered_features.append(best_feature)\n            best_feature = feature\n            last_selected_date = date\n        else:\n            if (\n                feature[\"properties\"][\"eo:cloud_cover\"]\n                &lt; best_feature[\"properties\"][\"eo:cloud_cover\"]\n            ):\n                best_feature = feature\n\n    # Handle the last period\n    if best_feature:\n        filtered_features.append(best_feature)\n\n    return filtered_features\n</code></pre>"}]}